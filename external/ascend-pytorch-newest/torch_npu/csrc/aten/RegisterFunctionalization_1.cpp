// @generated by gen_backend_stubs.py from RegisterFunctionalization.cpp

#include <ATen/core/LegacyTypeDispatch.h>
#include <ATen/EmptyTensor.h>
#include <ATen/FunctionalTensorWrapper.h>
#include <ATen/MemoryOverlap.h>
#include <torch/library.h>

#include <ATen/Operators.h>
#include <ATen/NativeFunctions.h>
#include "torch_npu/csrc/aten/CustomFunctions.h"
#include "torch_npu/csrc/core/npu/NPUException.h"


namespace at_npu {
namespace functionalization {

// This keyset is used by functionalization when it calls into meta kernels
// to accurately propagate stride metadata.
// Exclude any modes: the purpose of calling into meta kernels is only as an implementation
// detail to perform shape inference, and we don't want any modal keys to run.
// Specifically, we want to prevent functionalization and Python modes from running.
constexpr auto exclude_keys_for_meta_dispatch =
    c10::functorch_transforms_ks |
    c10::DispatchKeySet({
        c10::DispatchKey::FuncTorchDynamicLayerBackMode,
        c10::DispatchKey::FuncTorchDynamicLayerFrontMode,
        c10::DispatchKey::Python
    });

// Helper around at::has_internal_overlap.
// The ATen util is used in hot-path eager mode: it's always fast,
// but might return TOO_HARD sometimes.
// During functionalization, we're ok taking a bit longer
// to detect memory overlap.
inline bool has_internal_overlap_helper(const at::Tensor t)
{
    auto has_overlap = at::has_internal_overlap(t);
    if (has_overlap == at::MemOverlap::Yes) {
        return true;
    }
    if (has_overlap == at::MemOverlap::No) {
        return false;
    }
    return false;
}


inline at::Tensor to_meta(const at::Tensor& t)
{
    if (!t.defined()) return t;
    return at::native::empty_strided_meta_symint(t.sym_sizes(), t.sym_strides(),
        c10::make_optional(t.scalar_type()) /* dtype */, c10::make_optional(t.layout() /* layout */),
        c10::make_optional(c10::Device(at::kMeta)) /* device */, c10::nullopt /* pin_memory */);
}

inline c10::optional<at::Tensor> to_meta(const c10::optional<at::Tensor>& t)
{
    if (t.has_value()) {
        return c10::make_optional<at::Tensor>(to_meta(*t));
    }
    return c10::nullopt;
}

inline std::vector<at::Tensor> to_meta(at::ITensorListRef t_list)
{
    std::vector<at::Tensor> outputs;
    outputs.reserve(t_list.size());
    for (const auto& tensor : t_list) {
        outputs.push_back(to_meta(tensor));
    }
    return outputs;
}

inline c10::List<at::Tensor> to_meta(const c10::List<at::Tensor>& t_list)
{
    c10::List<at::Tensor> outputs;
    outputs.reserve(t_list.size());
    for (const auto i : c10::irange(t_list.size())) {
        outputs.push_back(to_meta(t_list[i]));
    }
    return outputs;
}

inline c10::List<c10::optional<at::Tensor>> to_meta(const c10::List<c10::optional<at::Tensor>>& t_list)
{
    c10::List<c10::optional<at::Tensor>> outputs;
    outputs.reserve(t_list.size());
    for (const auto i : c10::irange(t_list.size())) {
        outputs.push_back(to_meta(t_list[i]));
    }
    return outputs;
}



at::Tensor & npu_broadcast_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef size, at::Tensor & out) {
    if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technicaly all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at_npu::native::custom_ops::npu_broadcast_out(self_meta, size, out_meta);
    }

      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
    if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        if (false || at::functionalization::impl::isFunctionalTensor(self)) {
            // case 1: trying to mutate a non functional tensor with a functional tensor is an error
            TORCH_INTERNAL_ASSERT(false,
            "mutating a non-functional tensor with a functional tensor is not allowed.",
            " Please ensure that all of your inputs are wrapped inside of a functionalize() call.", PTA_ERROR(ErrCode::PARAM));
        } else {
            // case 2: arguments are not functional tensors, so we no-op and redispatch.
            at::AutoDispatchSkipFunctionalize guard;
            at::Tensor tmp_output = at_npu::native::custom_ops::npu_broadcast_out(self_, size, out_);
            return out;
        }
    } else {
        at::Tensor tmp_output;
        {
            at::AutoDispatchSkipFunctionalize guard;
            tmp_output = at_npu::native::custom_ops::npu_broadcast(self_, size);
        }
        at::functionalization::impl::replace_(out, tmp_output);
        at::functionalization::impl::commit_update(out);
        at::functionalization::impl::sync(out);
        return out;
    }
}

at::Tensor & npu_conv2d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const ::std::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups, at::Tensor & out) {
    if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technicaly all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at_npu::native::custom_ops::npu_conv2d_out(input_meta, weight_meta, bias_meta, stride, padding, dilation, groups, out_meta);
    }

      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      ::std::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
    if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        if (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias)) {
            // case 1: trying to mutate a non functional tensor with a functional tensor is an error
            TORCH_INTERNAL_ASSERT(false,
            "mutating a non-functional tensor with a functional tensor is not allowed.",
            " Please ensure that all of your inputs are wrapped inside of a functionalize() call.", PTA_ERROR(ErrCode::PARAM));
        } else {
            // case 2: arguments are not functional tensors, so we no-op and redispatch.
            at::AutoDispatchSkipFunctionalize guard;
            at::Tensor tmp_output = at_npu::native::custom_ops::npu_conv2d_out(input_, weight_, bias_, stride, padding, dilation, groups, out_);
            return out;
        }
    } else {
        at::Tensor tmp_output;
        {
            at::AutoDispatchSkipFunctionalize guard;
            tmp_output = at_npu::native::custom_ops::npu_conv2d(input_, weight_, bias_, stride, padding, dilation, groups);
        }
        at::functionalization::impl::replace_(out, tmp_output);
        at::functionalization::impl::commit_update(out);
        at::functionalization::impl::sync(out);
        return out;
    }
}

at::Tensor & npu_conv3d_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & input, const at::Tensor & weight, const ::std::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, int64_t groups, at::Tensor & out) {
    if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technicaly all support meta tensors).
        auto input_meta = to_meta(input);
        auto weight_meta = to_meta(weight);
        auto bias_meta = to_meta(bias);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at_npu::native::custom_ops::npu_conv3d_out(input_meta, weight_meta, bias_meta, stride, padding, dilation, groups, out_meta);
    }

      at::Tensor input_;
      if (at::functionalization::impl::isFunctionalTensor(input)) {
        at::functionalization::impl::sync(input);
        input_ = at::functionalization::impl::from_functional_tensor(input);
      } else {
        input_ = input;
      }
      
      at::Tensor weight_;
      if (at::functionalization::impl::isFunctionalTensor(weight)) {
        at::functionalization::impl::sync(weight);
        weight_ = at::functionalization::impl::from_functional_tensor(weight);
      } else {
        weight_ = weight;
      }
      
      ::std::optional<at::Tensor> bias_;
      if (at::functionalization::impl::isFunctionalTensor(bias)) {
        at::functionalization::impl::sync(bias);
        bias_ = at::functionalization::impl::from_functional_tensor(bias);
      } else {
        bias_ = bias;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
    if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        if (false || at::functionalization::impl::isFunctionalTensor(input) || at::functionalization::impl::isFunctionalTensor(weight) || at::functionalization::impl::isFunctionalTensor(bias)) {
            // case 1: trying to mutate a non functional tensor with a functional tensor is an error
            TORCH_INTERNAL_ASSERT(false,
            "mutating a non-functional tensor with a functional tensor is not allowed.",
            " Please ensure that all of your inputs are wrapped inside of a functionalize() call.", PTA_ERROR(ErrCode::PARAM));
        } else {
            // case 2: arguments are not functional tensors, so we no-op and redispatch.
            at::AutoDispatchSkipFunctionalize guard;
            at::Tensor tmp_output = at_npu::native::custom_ops::npu_conv3d_out(input_, weight_, bias_, stride, padding, dilation, groups, out_);
            return out;
        }
    } else {
        at::Tensor tmp_output;
        {
            at::AutoDispatchSkipFunctionalize guard;
            tmp_output = at_npu::native::custom_ops::npu_conv3d(input_, weight_, bias_, stride, padding, dilation, groups);
        }
        at::functionalization::impl::replace_(out, tmp_output);
        at::functionalization::impl::commit_update(out);
        at::functionalization::impl::sync(out);
        return out;
    }
}

at::Tensor & npu_indexing_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef begin, at::IntArrayRef end, at::IntArrayRef strides, int64_t begin_mask, int64_t end_mask, int64_t ellipsis_mask, int64_t new_axis_mask, int64_t shrink_axis_mask, at::Tensor & out) {
    if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technicaly all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at_npu::native::custom_ops::npu_indexing_out(self_meta, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, out_meta);
    }

      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
    if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        if (false || at::functionalization::impl::isFunctionalTensor(self)) {
            // case 1: trying to mutate a non functional tensor with a functional tensor is an error
            TORCH_INTERNAL_ASSERT(false,
            "mutating a non-functional tensor with a functional tensor is not allowed.",
            " Please ensure that all of your inputs are wrapped inside of a functionalize() call.", PTA_ERROR(ErrCode::PARAM));
        } else {
            // case 2: arguments are not functional tensors, so we no-op and redispatch.
            at::AutoDispatchSkipFunctionalize guard;
            at::Tensor tmp_output = at_npu::native::custom_ops::npu_indexing_out(self_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, out_);
            return out;
        }
    } else {
        at::Tensor tmp_output;
        {
            at::AutoDispatchSkipFunctionalize guard;
            tmp_output = at_npu::native::custom_ops::npu_indexing(self_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask);
        }
        at::functionalization::impl::replace_(out, tmp_output);
        at::functionalization::impl::commit_update(out);
        at::functionalization::impl::sync(out);
        return out;
    }
}

at::Tensor & npu_quant_scatter_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & indices, const at::Tensor & updates, const at::Tensor & quant_scales, const ::std::optional<at::Tensor> & quant_zero_points, int64_t axis, int64_t quant_axis, c10::string_view reduce) {
    if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technicaly all support meta tensors).
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        auto updates_meta = to_meta(updates);
        auto quant_scales_meta = to_meta(quant_scales);
        auto quant_zero_points_meta = to_meta(quant_zero_points);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at_npu::native::custom_ops::npu_quant_scatter_(self_meta, indices_meta, updates_meta, quant_scales_meta, quant_zero_points_meta, axis, quant_axis, reduce);
    }

      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor updates_;
      if (at::functionalization::impl::isFunctionalTensor(updates)) {
        at::functionalization::impl::sync(updates);
        updates_ = at::functionalization::impl::from_functional_tensor(updates);
      } else {
        updates_ = updates;
      }
      
      at::Tensor quant_scales_;
      if (at::functionalization::impl::isFunctionalTensor(quant_scales)) {
        at::functionalization::impl::sync(quant_scales);
        quant_scales_ = at::functionalization::impl::from_functional_tensor(quant_scales);
      } else {
        quant_scales_ = quant_scales;
      }
      
      ::std::optional<at::Tensor> quant_zero_points_;
      if (at::functionalization::impl::isFunctionalTensor(quant_zero_points)) {
        at::functionalization::impl::sync(quant_zero_points);
        quant_zero_points_ = at::functionalization::impl::from_functional_tensor(quant_zero_points);
      } else {
        quant_zero_points_ = quant_zero_points;
      }
    if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        if (false || at::functionalization::impl::isFunctionalTensor(indices) || at::functionalization::impl::isFunctionalTensor(updates) || at::functionalization::impl::isFunctionalTensor(quant_scales) || at::functionalization::impl::isFunctionalTensor(quant_zero_points)) {
            // case 1: trying to mutate a non functional tensor with a functional tensor is an error
            TORCH_INTERNAL_ASSERT(false,
            "mutating a non-functional tensor with a functional tensor is not allowed.",
            " Please ensure that all of your inputs are wrapped inside of a functionalize() call.", PTA_ERROR(ErrCode::PARAM));
        } else {
            // case 2: arguments are not functional tensors, so we no-op and redispatch.
            at::AutoDispatchSkipFunctionalize guard;
            at::Tensor tmp_output = at_npu::native::custom_ops::npu_quant_scatter_(self_, indices_, updates_, quant_scales_, quant_zero_points_, axis, quant_axis, reduce);
            return self;
        }
    } else {
        at::Tensor tmp_output;
        {
            at::AutoDispatchSkipFunctionalize guard;
            tmp_output = at_npu::native::custom_ops::npu_quant_scatter(self_, indices_, updates_, quant_scales_, quant_zero_points_, axis, quant_axis, reduce);
        }
        at::functionalization::impl::replace_(self, tmp_output);
        at::functionalization::impl::commit_update(self);
        at::functionalization::impl::sync(self);
        return self;
    }
}

void npu_scatter_list_(c10::DispatchKeySet dispatchKeySet, at::TensorList self, const at::Tensor & indices, const at::Tensor & updates, const ::std::optional<at::Tensor> & mask, c10::string_view reduce, int64_t axis) {
    if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technicaly all support meta tensors).
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        auto updates_meta = to_meta(updates);
        auto mask_meta = to_meta(mask);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at_npu::native::custom_ops::npu_scatter_list_(self_meta, indices_meta, updates_meta, mask_meta, reduce, axis);
    }

      ::std::vector<at::Tensor> self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self.vec();
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor updates_;
      if (at::functionalization::impl::isFunctionalTensor(updates)) {
        at::functionalization::impl::sync(updates);
        updates_ = at::functionalization::impl::from_functional_tensor(updates);
      } else {
        updates_ = updates;
      }
      
      ::std::optional<at::Tensor> mask_;
      if (at::functionalization::impl::isFunctionalTensor(mask)) {
        at::functionalization::impl::sync(mask);
        mask_ = at::functionalization::impl::from_functional_tensor(mask);
      } else {
        mask_ = mask;
      }
    if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        if (false || at::functionalization::impl::isFunctionalTensor(indices) || at::functionalization::impl::isFunctionalTensor(updates) || at::functionalization::impl::isFunctionalTensor(mask)) {
            // case 1: trying to mutate a non functional tensor with a functional tensor is an error
            TORCH_INTERNAL_ASSERT(false,
            "mutating a non-functional tensor with a functional tensor is not allowed.",
            " Please ensure that all of your inputs are wrapped inside of a functionalize() call.", PTA_ERROR(ErrCode::PARAM));
        } else {
            // case 2: arguments are not functional tensors, so we no-op and redispatch.
            at::AutoDispatchSkipFunctionalize guard;
            at_npu::native::custom_ops::npu_scatter_list_(self_, indices_, updates_, mask_, reduce, axis);
            
        }
    } else {
        ::std::vector<at::Tensor> tmp_output;
        {
            at::AutoDispatchSkipFunctionalize guard;
            tmp_output = at_npu::native::custom_ops::npu_scatter_list(self_, indices_, updates_, mask_, reduce, axis);
        }
        at::functionalization::impl::replace_(self, tmp_output);
        at::functionalization::impl::commit_update(self);
        at::functionalization::impl::sync(self);
        
    }
}

at::Tensor & npu_silu_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self) {
    if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technicaly all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at_npu::native::custom_ops::npu_silu_(self_meta);
    }

      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
    if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        if (false) {
            // case 1: trying to mutate a non functional tensor with a functional tensor is an error
            TORCH_INTERNAL_ASSERT(false,
            "mutating a non-functional tensor with a functional tensor is not allowed.",
            " Please ensure that all of your inputs are wrapped inside of a functionalize() call.", PTA_ERROR(ErrCode::PARAM));
        } else {
            // case 2: arguments are not functional tensors, so we no-op and redispatch.
            at::AutoDispatchSkipFunctionalize guard;
            at::Tensor tmp_output = at_npu::native::custom_ops::npu_silu_(self_);
            return self;
        }
    } else {
        at::Tensor tmp_output;
        {
            at::AutoDispatchSkipFunctionalize guard;
            tmp_output = at_npu::native::custom_ops::npu_silu(self_);
        }
        at::functionalization::impl::replace_(self, tmp_output);
        at::functionalization::impl::commit_update(self);
        at::functionalization::impl::sync(self);
        return self;
    }
}

at::Tensor & npu_transpose_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef perm, bool require_contiguous, at::Tensor & out) {
    if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technicaly all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at_npu::native::custom_ops::npu_transpose_out(self_meta, perm, require_contiguous, out_meta);
    }

      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
    if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        if (false || at::functionalization::impl::isFunctionalTensor(self)) {
            // case 1: trying to mutate a non functional tensor with a functional tensor is an error
            TORCH_INTERNAL_ASSERT(false,
            "mutating a non-functional tensor with a functional tensor is not allowed.",
            " Please ensure that all of your inputs are wrapped inside of a functionalize() call.", PTA_ERROR(ErrCode::PARAM));
        } else {
            // case 2: arguments are not functional tensors, so we no-op and redispatch.
            at::AutoDispatchSkipFunctionalize guard;
            at::Tensor tmp_output = at_npu::native::custom_ops::npu_transpose_out(self_, perm, require_contiguous, out_);
            return out;
        }
    } else {
        at::Tensor tmp_output;
        {
            at::AutoDispatchSkipFunctionalize guard;
            tmp_output = at_npu::native::custom_ops::npu_transpose(self_, perm, require_contiguous);
        }
        at::functionalization::impl::replace_(out, tmp_output);
        at::functionalization::impl::commit_update(out);
        at::functionalization::impl::sync(out);
        return out;
    }
}

at::Tensor & scatter_update_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & indices, const at::Tensor & updates, int64_t axis) {
    if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technicaly all support meta tensors).
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        auto updates_meta = to_meta(updates);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at_npu::native::custom_ops::scatter_update_(self_meta, indices_meta, updates_meta, axis);
    }

      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor updates_;
      if (at::functionalization::impl::isFunctionalTensor(updates)) {
        at::functionalization::impl::sync(updates);
        updates_ = at::functionalization::impl::from_functional_tensor(updates);
      } else {
        updates_ = updates;
      }
    if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        if (false || at::functionalization::impl::isFunctionalTensor(indices) || at::functionalization::impl::isFunctionalTensor(updates)) {
            // case 1: trying to mutate a non functional tensor with a functional tensor is an error
            TORCH_INTERNAL_ASSERT(false,
            "mutating a non-functional tensor with a functional tensor is not allowed.",
            " Please ensure that all of your inputs are wrapped inside of a functionalize() call.", PTA_ERROR(ErrCode::PARAM));
        } else {
            // case 2: arguments are not functional tensors, so we no-op and redispatch.
            at::AutoDispatchSkipFunctionalize guard;
            at::Tensor tmp_output = at_npu::native::custom_ops::scatter_update_(self_, indices_, updates_, axis);
            return self;
        }
    } else {
        at::Tensor tmp_output;
        {
            at::AutoDispatchSkipFunctionalize guard;
            tmp_output = at_npu::native::custom_ops::scatter_update(self_, indices_, updates_, axis);
        }
        at::functionalization::impl::replace_(self, tmp_output);
        at::functionalization::impl::commit_update(self);
        at::functionalization::impl::sync(self);
        return self;
    }
}

}  // namespace functionalization

namespace {

TORCH_LIBRARY_IMPL(npu, Functionalize, m) {
    m.impl("npu_broadcast.out", TORCH_FN(functionalization::npu_broadcast_out_out));
    m.impl("npu_conv2d.out", TORCH_FN(functionalization::npu_conv2d_out_out));
    m.impl("npu_conv3d.out", TORCH_FN(functionalization::npu_conv3d_out_out));
    m.impl("npu_indexing.out", TORCH_FN(functionalization::npu_indexing_out_out));
    m.impl("npu_quant_scatter_", TORCH_FN(functionalization::npu_quant_scatter_));
    m.impl("npu_scatter_list_", TORCH_FN(functionalization::npu_scatter_list_));
    m.impl("npu_silu_", TORCH_FN(functionalization::npu_silu_));
    m.impl("npu_transpose.out", TORCH_FN(functionalization::npu_transpose_out_out));
    m.impl("scatter_update_", TORCH_FN(functionalization::scatter_update_));;
}

}  // namespace

} // namespace at_npu
