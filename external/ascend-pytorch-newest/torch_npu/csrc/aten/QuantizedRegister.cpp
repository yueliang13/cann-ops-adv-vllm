// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#ifndef __STDC_FORMAT_MACROS
#define __STDC_FORMAT_MACROS
#endif

// an external backend might generate file within its code tree
// and check all the source files within the tree with clang-format.
// so, disable it since the backend might have a different config.
// clang-format off

// NOTE: This condition is true for all PyTorch internal libraries, it
//       just excludes external projects such as torch_xla which
//       re-use some of the PyTorch codegen machinery.
#if defined(CAFFE2_BUILD_MAIN_LIB)        || \
    defined(TORCH_CUDA_BUILD_MAIN_LIB)    || \
    defined(TORCH_HIP_BUILD_MAIN_LIB)     || \
    defined(TORCH_CUDA_CU_BUILD_MAIN_LIB) || \
    defined(TORCH_CUDA_CPP_BUILD_MAIN_LIB)
#define TORCH_ASSERT_ONLY_METHOD_OPERATORS
#endif

// @generated by gen_backend_stubs.py from RegisterDispatchKey.cpp

#include <c10/core/TensorImpl.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/ExclusivelyOwned.h>
#include <c10/util/Half.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <optional>
#include <ATen/Tensor.h>
#include <ATen/native/Resize.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/core/op_registration/adaption.h>
#include <torch/library.h>

#include <ATen/ops/quantize_per_tensor.h>
#include "op_plugin/OpInterface.h"



// See template file RegisterDispatchDefinitions.ini
namespace at {
// NB: TORCH_LIBRARY_IMPL must be in an anonymous namespace to avoid
// ambiguity with conflicting identifiers that may have been defined in
// at namespace already.
namespace {
TORCH_LIBRARY_IMPL(aten, QuantizedPrivateUse1, m) {
m.impl("dequantize.self", TORCH_FN(op_plugin::dequantize));
m.impl("int_repr", TORCH_FN(op_plugin::int_repr));
m.impl("_empty_affine_quantized", TORCH_FN(op_plugin::_empty_affine_quantized));
m.impl("q_scale", TORCH_FN(at::native::q_scale_quant));
m.impl("q_per_channel_scales", TORCH_FN(at::native::q_per_channel_scales));
m.impl("q_zero_point", TORCH_FN(at::native::q_zero_point_quant));
m.impl("q_per_channel_zero_points", TORCH_FN(at::native::q_per_channel_zero_points));
m.impl("q_per_channel_axis", TORCH_FN(at::native::q_per_channel_axis));
m.impl("qscheme", TORCH_FN(at::native::qscheme_quant));
};
} // anonymous namespace
namespace  {
} // namespace 
} // namespace at
