// @generated by gen_backend_stubs.py from RegisterFunctionalization.cpp

#include <ATen/core/LegacyTypeDispatch.h>
#include <ATen/EmptyTensor.h>
#include <ATen/FunctionalTensorWrapper.h>
#include <ATen/MemoryOverlap.h>
#include <torch/library.h>

#include <ATen/Operators.h>
#include <ATen/NativeFunctions.h>
#include "torch_npu/csrc/aten/CustomFunctions.h"
#include "torch_npu/csrc/core/npu/NPUException.h"


namespace at_npu {
namespace functionalization {

// This keyset is used by functionalization when it calls into meta kernels
// to accurately propagate stride metadata.
// Exclude any modes: the purpose of calling into meta kernels is only as an implementation
// detail to perform shape inference, and we don't want any modal keys to run.
// Specifically, we want to prevent functionalization and Python modes from running.
constexpr auto exclude_keys_for_meta_dispatch =
    c10::functorch_transforms_ks |
    c10::DispatchKeySet({
        c10::DispatchKey::FuncTorchDynamicLayerBackMode,
        c10::DispatchKey::FuncTorchDynamicLayerFrontMode,
        c10::DispatchKey::Python
    });

// Helper around at::has_internal_overlap.
// The ATen util is used in hot-path eager mode: it's always fast,
// but might return TOO_HARD sometimes.
// During functionalization, we're ok taking a bit longer
// to detect memory overlap.
inline bool has_internal_overlap_helper(const at::Tensor t)
{
    auto has_overlap = at::has_internal_overlap(t);
    if (has_overlap == at::MemOverlap::Yes) {
        return true;
    }
    if (has_overlap == at::MemOverlap::No) {
        return false;
    }
    return false;
}


inline at::Tensor to_meta(const at::Tensor& t)
{
    if (!t.defined()) return t;
    return at::native::empty_strided_meta_symint(t.sym_sizes(), t.sym_strides(),
        c10::make_optional(t.scalar_type()) /* dtype */, c10::make_optional(t.layout() /* layout */),
        c10::make_optional(c10::Device(at::kMeta)) /* device */, c10::nullopt /* pin_memory */);
}

inline c10::optional<at::Tensor> to_meta(const c10::optional<at::Tensor>& t)
{
    if (t.has_value()) {
        return c10::make_optional<at::Tensor>(to_meta(*t));
    }
    return c10::nullopt;
}

inline std::vector<at::Tensor> to_meta(at::ITensorListRef t_list)
{
    std::vector<at::Tensor> outputs;
    outputs.reserve(t_list.size());
    for (const auto& tensor : t_list) {
        outputs.push_back(to_meta(tensor));
    }
    return outputs;
}

inline c10::List<at::Tensor> to_meta(const c10::List<at::Tensor>& t_list)
{
    c10::List<at::Tensor> outputs;
    outputs.reserve(t_list.size());
    for (const auto i : c10::irange(t_list.size())) {
        outputs.push_back(to_meta(t_list[i]));
    }
    return outputs;
}

inline c10::List<c10::optional<at::Tensor>> to_meta(const c10::List<c10::optional<at::Tensor>>& t_list)
{
    c10::List<c10::optional<at::Tensor>> outputs;
    outputs.reserve(t_list.size());
    for (const auto i : c10::irange(t_list.size())) {
        outputs.push_back(to_meta(t_list[i]));
    }
    return outputs;
}



at::Tensor & npu_format_cast__acl_format(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, int64_t acl_format) {
    if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technicaly all support meta tensors).
        auto self_meta = to_meta(self);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at_npu::native::custom_ops::npu_format_cast_(self_meta, acl_format);
    }

      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
    if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        if (false) {
            // case 1: trying to mutate a non functional tensor with a functional tensor is an error
            TORCH_INTERNAL_ASSERT(false,
            "mutating a non-functional tensor with a functional tensor is not allowed.",
            " Please ensure that all of your inputs are wrapped inside of a functionalize() call.", PTA_ERROR(ErrCode::PARAM));
        } else {
            // case 2: arguments are not functional tensors, so we no-op and redispatch.
            at::AutoDispatchSkipFunctionalize guard;
            at::Tensor tmp_output = at_npu::native::custom_ops::npu_format_cast_(self_, acl_format);
            return self;
        }
    } else {
        at::Tensor tmp_output;
        {
            at::AutoDispatchSkipFunctionalize guard;
            tmp_output = at_npu::native::custom_ops::npu_format_cast(self_, acl_format);
        }
        at::functionalization::impl::replace_(self, tmp_output);
        at::functionalization::impl::commit_update(self);
        at::functionalization::impl::sync(self);
        return self;
    }
}

at::Tensor & npu_batch_gather_matmul_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & x, const at::Tensor & weight_b, const at::Tensor & indices, const ::std::optional<at::Tensor> & weight_a, int64_t layer_idx, double scale, int64_t y_offset, int64_t y_slice_size) {
    if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technicaly all support meta tensors).
        auto self_meta = to_meta(self);
        auto x_meta = to_meta(x);
        auto weight_b_meta = to_meta(weight_b);
        auto indices_meta = to_meta(indices);
        auto weight_a_meta = to_meta(weight_a);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at_npu::native::custom_ops::npu_batch_gather_matmul_(self_meta, x_meta, weight_b_meta, indices_meta, weight_a_meta, layer_idx, scale, y_offset, y_slice_size);
    }

      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor x_;
      if (at::functionalization::impl::isFunctionalTensor(x)) {
        at::functionalization::impl::sync(x);
        x_ = at::functionalization::impl::from_functional_tensor(x);
      } else {
        x_ = x;
      }
      
      at::Tensor weight_b_;
      if (at::functionalization::impl::isFunctionalTensor(weight_b)) {
        at::functionalization::impl::sync(weight_b);
        weight_b_ = at::functionalization::impl::from_functional_tensor(weight_b);
      } else {
        weight_b_ = weight_b;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      ::std::optional<at::Tensor> weight_a_;
      if (at::functionalization::impl::isFunctionalTensor(weight_a)) {
        at::functionalization::impl::sync(weight_a);
        weight_a_ = at::functionalization::impl::from_functional_tensor(weight_a);
      } else {
        weight_a_ = weight_a;
      }
    if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        if (false || at::functionalization::impl::isFunctionalTensor(x) || at::functionalization::impl::isFunctionalTensor(weight_b) || at::functionalization::impl::isFunctionalTensor(indices) || at::functionalization::impl::isFunctionalTensor(weight_a)) {
            // case 1: trying to mutate a non functional tensor with a functional tensor is an error
            TORCH_INTERNAL_ASSERT(false,
            "mutating a non-functional tensor with a functional tensor is not allowed.",
            " Please ensure that all of your inputs are wrapped inside of a functionalize() call.", PTA_ERROR(ErrCode::PARAM));
        } else {
            // case 2: arguments are not functional tensors, so we no-op and redispatch.
            at::AutoDispatchSkipFunctionalize guard;
            at::Tensor tmp_output = at_npu::native::custom_ops::npu_batch_gather_matmul_(self_, x_, weight_b_, indices_, weight_a_, layer_idx, scale, y_offset, y_slice_size);
            return self;
        }
    } else {
        at::Tensor tmp_output;
        {
            at::AutoDispatchSkipFunctionalize guard;
            tmp_output = at_npu::native::custom_ops::npu_batch_gather_matmul(self_, x_, weight_b_, indices_, weight_a_, layer_idx, scale, y_offset, y_slice_size);
        }
        at::functionalization::impl::replace_(self, tmp_output);
        at::functionalization::impl::commit_update(self);
        at::functionalization::impl::sync(self);
        return self;
    }
}

at::Tensor & npu_reshape_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef shape, bool can_refresh, at::Tensor & out) {
    if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technicaly all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at_npu::native::custom_ops::npu_reshape_out(self_meta, shape, can_refresh, out_meta);
    }

      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
    if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        if (false || at::functionalization::impl::isFunctionalTensor(self)) {
            // case 1: trying to mutate a non functional tensor with a functional tensor is an error
            TORCH_INTERNAL_ASSERT(false,
            "mutating a non-functional tensor with a functional tensor is not allowed.",
            " Please ensure that all of your inputs are wrapped inside of a functionalize() call.", PTA_ERROR(ErrCode::PARAM));
        } else {
            // case 2: arguments are not functional tensors, so we no-op and redispatch.
            at::AutoDispatchSkipFunctionalize guard;
            at::Tensor tmp_output = at_npu::native::custom_ops::npu_reshape_out(self_, shape, can_refresh, out_);
            return out;
        }
    } else {
        at::Tensor tmp_output;
        {
            at::AutoDispatchSkipFunctionalize guard;
            tmp_output = at_npu::native::custom_ops::npu_reshape(self_, shape, can_refresh);
        }
        at::functionalization::impl::replace_(out, tmp_output);
        at::functionalization::impl::commit_update(out);
        at::functionalization::impl::sync(out);
        return out;
    }
}

at::Tensor & npu_scatter_nd_update_(c10::DispatchKeySet dispatchKeySet, at::Tensor & self, const at::Tensor & indices, const at::Tensor & updates) {
    if (true) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technicaly all support meta tensors).
        auto self_meta = to_meta(self);
        auto indices_meta = to_meta(indices);
        auto updates_meta = to_meta(updates);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at_npu::native::custom_ops::npu_scatter_nd_update_(self_meta, indices_meta, updates_meta);
    }

      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor indices_;
      if (at::functionalization::impl::isFunctionalTensor(indices)) {
        at::functionalization::impl::sync(indices);
        indices_ = at::functionalization::impl::from_functional_tensor(indices);
      } else {
        indices_ = indices;
      }
      
      at::Tensor updates_;
      if (at::functionalization::impl::isFunctionalTensor(updates)) {
        at::functionalization::impl::sync(updates);
        updates_ = at::functionalization::impl::from_functional_tensor(updates);
      } else {
        updates_ = updates;
      }
    if (!(true && at::functionalization::impl::isFunctionalTensor(self))) {
        if (false || at::functionalization::impl::isFunctionalTensor(indices) || at::functionalization::impl::isFunctionalTensor(updates)) {
            // case 1: trying to mutate a non functional tensor with a functional tensor is an error
            TORCH_INTERNAL_ASSERT(false,
            "mutating a non-functional tensor with a functional tensor is not allowed.",
            " Please ensure that all of your inputs are wrapped inside of a functionalize() call.", PTA_ERROR(ErrCode::PARAM));
        } else {
            // case 2: arguments are not functional tensors, so we no-op and redispatch.
            at::AutoDispatchSkipFunctionalize guard;
            at::Tensor tmp_output = at_npu::native::custom_ops::npu_scatter_nd_update_(self_, indices_, updates_);
            return self;
        }
    } else {
        at::Tensor tmp_output;
        {
            at::AutoDispatchSkipFunctionalize guard;
            tmp_output = at_npu::native::custom_ops::npu_scatter_nd_update(self_, indices_, updates_);
        }
        at::functionalization::impl::replace_(self, tmp_output);
        at::functionalization::impl::commit_update(self);
        at::functionalization::impl::sync(self);
        return self;
    }
}

at::Tensor & npu_slice_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef offsets, at::IntArrayRef size, at::Tensor & out) {
    if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technicaly all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at_npu::native::custom_ops::npu_slice_out(self_meta, offsets, size, out_meta);
    }

      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
    if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        if (false || at::functionalization::impl::isFunctionalTensor(self)) {
            // case 1: trying to mutate a non functional tensor with a functional tensor is an error
            TORCH_INTERNAL_ASSERT(false,
            "mutating a non-functional tensor with a functional tensor is not allowed.",
            " Please ensure that all of your inputs are wrapped inside of a functionalize() call.", PTA_ERROR(ErrCode::PARAM));
        } else {
            // case 2: arguments are not functional tensors, so we no-op and redispatch.
            at::AutoDispatchSkipFunctionalize guard;
            at::Tensor tmp_output = at_npu::native::custom_ops::npu_slice_out(self_, offsets, size, out_);
            return out;
        }
    } else {
        at::Tensor tmp_output;
        {
            at::AutoDispatchSkipFunctionalize guard;
            tmp_output = at_npu::native::custom_ops::npu_slice(self_, offsets, size);
        }
        at::functionalization::impl::replace_(out, tmp_output);
        at::functionalization::impl::commit_update(out);
        at::functionalization::impl::sync(out);
        return out;
    }
}

at::Tensor & npu_sort_v2_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, int64_t dim, bool descending, at::Tensor & out) {
    if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technicaly all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at_npu::native::custom_ops::npu_sort_v2_out(self_meta, dim, descending, out_meta);
    }

      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
    if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        if (false || at::functionalization::impl::isFunctionalTensor(self)) {
            // case 1: trying to mutate a non functional tensor with a functional tensor is an error
            TORCH_INTERNAL_ASSERT(false,
            "mutating a non-functional tensor with a functional tensor is not allowed.",
            " Please ensure that all of your inputs are wrapped inside of a functionalize() call.", PTA_ERROR(ErrCode::PARAM));
        } else {
            // case 2: arguments are not functional tensors, so we no-op and redispatch.
            at::AutoDispatchSkipFunctionalize guard;
            at::Tensor tmp_output = at_npu::native::custom_ops::npu_sort_v2_out(self_, dim, descending, out_);
            return out;
        }
    } else {
        at::Tensor tmp_output;
        {
            at::AutoDispatchSkipFunctionalize guard;
            tmp_output = at_npu::native::custom_ops::npu_sort_v2(self_, dim, descending);
        }
        at::functionalization::impl::replace_(out, tmp_output);
        at::functionalization::impl::commit_update(out);
        at::functionalization::impl::sync(out);
        return out;
    }
}

at::Tensor & npu_stride_copy_out_out(c10::DispatchKeySet dispatchKeySet, const at::Tensor & self, at::IntArrayRef shape, at::IntArrayRef stride, const at::Scalar & storage_offset, at::Tensor & out) {
    if (false) {
        // Before converting the mutable op to its functional variant, run meta tensors through the original op.
        // This will help us catch shape errors that apply to inplace ops that wouldn't apply to their functional variants.
        // (We can only do this for inplace ops today though, because they technicaly all support meta tensors).
        auto self_meta = to_meta(self);
        auto out_meta = to_meta(out);
        at::AutoDispatchSkipFunctionalize func_guard;
        c10::impl::ExcludeDispatchKeyGuard guard(exclude_keys_for_meta_dispatch);
        at_npu::native::custom_ops::npu_stride_copy_out(self_meta, shape, stride, storage_offset, out_meta);
    }

      at::Tensor self_;
      if (at::functionalization::impl::isFunctionalTensor(self)) {
        at::functionalization::impl::sync(self);
        self_ = at::functionalization::impl::from_functional_tensor(self);
      } else {
        self_ = self;
      }
      
      at::Tensor out_;
      if (at::functionalization::impl::isFunctionalTensor(out)) {
        at::functionalization::impl::sync(out);
        out_ = at::functionalization::impl::from_functional_tensor(out);
      } else {
        out_ = out;
      }
    if (!(true && at::functionalization::impl::isFunctionalTensor(out))) {
        if (false || at::functionalization::impl::isFunctionalTensor(self)) {
            // case 1: trying to mutate a non functional tensor with a functional tensor is an error
            TORCH_INTERNAL_ASSERT(false,
            "mutating a non-functional tensor with a functional tensor is not allowed.",
            " Please ensure that all of your inputs are wrapped inside of a functionalize() call.", PTA_ERROR(ErrCode::PARAM));
        } else {
            // case 2: arguments are not functional tensors, so we no-op and redispatch.
            at::AutoDispatchSkipFunctionalize guard;
            at::Tensor tmp_output = at_npu::native::custom_ops::npu_stride_copy_out(self_, shape, stride, storage_offset, out_);
            return out;
        }
    } else {
        at::Tensor tmp_output;
        {
            at::AutoDispatchSkipFunctionalize guard;
            tmp_output = at_npu::native::custom_ops::npu_stride_copy(self_, shape, stride, storage_offset);
        }
        at::functionalization::impl::replace_(out, tmp_output);
        at::functionalization::impl::commit_update(out);
        at::functionalization::impl::sync(out);
        return out;
    }
}

}  // namespace functionalization

namespace {

TORCH_LIBRARY_IMPL(npu, Functionalize, m) {
    m.impl("npu_format_cast_.acl_format", TORCH_FN(functionalization::npu_format_cast__acl_format));
    m.impl("npu_batch_gather_matmul_", TORCH_FN(functionalization::npu_batch_gather_matmul_));
    m.impl("npu_reshape.out", TORCH_FN(functionalization::npu_reshape_out_out));
    m.impl("npu_scatter_nd_update_", TORCH_FN(functionalization::npu_scatter_nd_update_));
    m.impl("npu_slice.out", TORCH_FN(functionalization::npu_slice_out_out));
    m.impl("npu_sort_v2.out", TORCH_FN(functionalization::npu_sort_v2_out_out));
    m.impl("npu_stride_copy.out", TORCH_FN(functionalization::npu_stride_copy_out_out));;
}

}  // namespace

} // namespace at_npu
