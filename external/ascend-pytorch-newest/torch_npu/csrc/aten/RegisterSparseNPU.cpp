// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#ifndef __STDC_FORMAT_MACROS
#define __STDC_FORMAT_MACROS
#endif

// an external backend might generate file within its code tree
// and check all the source files within the tree with clang-format.
// so, disable it since the backend might have a different config.
// clang-format off

// NOTE: This condition is true for all PyTorch internal libraries, it
//       just excludes external projects such as torch_xla which
//       re-use some of the PyTorch codegen machinery.
#if defined(CAFFE2_BUILD_MAIN_LIB)        || \
    defined(TORCH_CUDA_BUILD_MAIN_LIB)    || \
    defined(TORCH_HIP_BUILD_MAIN_LIB)     || \
    defined(TORCH_CUDA_CU_BUILD_MAIN_LIB) || \
    defined(TORCH_CUDA_CPP_BUILD_MAIN_LIB)
#define TORCH_ASSERT_ONLY_METHOD_OPERATORS
#endif

// @generated by gen_backend_stubs.py from RegisterDispatchKey.cpp

#include <c10/core/TensorImpl.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/ExclusivelyOwned.h>
#include <c10/util/Half.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <optional>
#include <ATen/Tensor.h>
#include <ATen/native/Resize.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/core/op_registration/adaption.h>
#include <torch/library.h>

#include "torch_npu/csrc/core/npu/NPURecovery.h"
#ifndef BUILD_LIBTORCH
#include "torch_npu/csrc/profiler/utils.h"
#endif

#include "torch_npu/csrc/aten/NPUNativeFunctions.h"
#include "torch_npu/csrc/framework/interface/EnvVariables.h"
#include "torch_npu/csrc/aten/NPUOpApiNativeFunctions.h"
#include "torch_npu/csrc/framework/FormatHelper.h"
#include "torch_npu/csrc/framework/utils/ForceAclnnList.h"
#include "op_plugin/OpInterface.h"
#include <ATen/NativeFunctions.h>
#include <ATen/Functions.h>
#include <c10/macros/Macros.h>


// See template file RegisterDispatchDefinitions.ini
namespace at {
// NB: TORCH_LIBRARY_IMPL must be in an anonymous namespace to avoid
// ambiguity with conflicting identifiers that may have been defined in
// at namespace already.
namespace {
C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED("-Wunused-function")
void resize_out(const Tensor &out, IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {
  TORCH_CHECK(options.dtype() == out.dtype(),
      "Expected out tensor to have dtype ", options.dtype(), ", but got ", out.dtype(), " instead");
  TORCH_CHECK(options.device() == out.device(),
      "Expected out tensor to have device ", options.device(), ", but got ", out.device(), " instead");
  const bool resized = at::native::resize_output(out, sizes);
  // Only restride if a resize occurred; otherwise we ignore the (advisory)
  // strides from the meta function and directly use the output tensor's
  // preexisting strides
  if (resized) {
    if (!strides.empty()) {
      TORCH_INTERNAL_ASSERT(!options.memory_format_opt().has_value());
      // TODO: avoid the redispatch here
      out.as_strided_(sizes, strides);
    } else if (options.memory_format_opt().has_value()) {
      out.unsafeGetTensorImpl()->empty_tensor_restride(*options.memory_format_opt());
    }
  }
}
void check_inplace(const Tensor &self, IntArrayRef sizes, const TensorOptions &options) {
  // These checks are needed on those operators that:
  //   1) don't use 'TensorIterator' (e.g. 'addmm' and 'baddbmm')
  //   2) have particular typing rules (e.g. 'cumsum' and 'cumprod')
  // For other operators (e.g. 'add'), 'TensorIterator' already checks
  // these things separately.
  TORCH_CHECK(options.dtype() == self.dtype(),
      "Bad in-place call: ",
      "input tensor dtype ", self.dtype(), " and output tensor dtype ", options.dtype(), " should match");
  TORCH_CHECK(options.device() == self.device(),
      "Bad in-place call: ",
      "input tensor device ", self.device(), " and output tensor device ", options.device(), " should match");
  TORCH_CHECK(sizes == self.sizes(),
      "Bad in-place call: ",
      "input tensor size ", self.sizes(), " and output tensor size ", sizes, " should match");
}
C10_DIAGNOSTIC_POP()
namespace {
at::Tensor & wrapper_SparseNPU_out_add_out(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
  // No device check
if (c10_npu::get_npu_data_unsafe_flag()) {
    c10_npu::check_npu_tensor_is_safe(out);
    c10_npu::check_npu_tensor_is_safe(self);
    c10_npu::check_npu_tensor_is_safe(other);
}
const OptionalDeviceGuard device_guard(device_of(self));
#ifndef BUILD_LIBTORCH
torch_npu::profiler::NPURecordFunction guard;
#endif
    return op_plugin::add_out_sparse(self, other, alpha, out);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseNPU___coalesce(const at::Tensor & self) {
c10::optional<at::Device> common_device = at::nullopt;
(void)common_device; // Suppress unused variable warning
c10::impl::check_and_update_common_device(common_device, self, "wrapper_SparseNPU___coalesce", "self");
if (c10_npu::get_npu_data_unsafe_flag()) {
    c10_npu::check_npu_tensor_is_safe(self);
}
const OptionalDeviceGuard device_guard(device_of(self));
#ifndef BUILD_LIBTORCH
torch_npu::profiler::NPURecordFunction guard;
#endif
    return op_plugin::_coalesce_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_SparseNPU__max(const at::Tensor & self) {
  // No device check
if (c10_npu::get_npu_data_unsafe_flag()) {
    c10_npu::check_npu_tensor_is_safe(self);
}
const OptionalDeviceGuard device_guard(device_of(self));
#ifndef BUILD_LIBTORCH
torch_npu::profiler::NPURecordFunction guard;
#endif
    return op_plugin::max_sparse(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_SparseNPU_out_max_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
  // No device check
if (c10_npu::get_npu_data_unsafe_flag()) {
    c10_npu::check_npu_tensor_is_safe(out);
    c10_npu::check_npu_tensor_is_safe(self);
    c10_npu::check_npu_tensor_is_safe(other);
}
const OptionalDeviceGuard device_guard(device_of(self));
#ifndef BUILD_LIBTORCH
torch_npu::profiler::NPURecordFunction guard;
#endif
    return op_plugin::max_out_sparse(self, other, out);
}
} // anonymous namespace
TORCH_LIBRARY_IMPL(aten, SparsePrivateUse1, m) {
m.impl("add.out",
TORCH_FN(wrapper_SparseNPU_out_add_out));
m.impl("_coalesce",
TORCH_FN(wrapper_SparseNPU___coalesce));
m.impl("max",
TORCH_FN(wrapper_SparseNPU__max));
m.impl("max.out",
TORCH_FN(wrapper_SparseNPU_out_max_out));
};
} // anonymous namespace
namespace sparsenpu {
namespace {
at::Tensor wrap_SparseNPU_abs_(const at::Tensor & self) {
    return at::native::abs_sparse(self);
}
at::Tensor & wrap_SparseNPU_abs__(at::Tensor & self) {
    return at::native::abs_sparse_(self);
}
at::Tensor & wrap_SparseNPU_abs_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::abs_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_sgn_(const at::Tensor & self) {
    return at::native::sgn_sparse(self);
}
at::Tensor & wrap_SparseNPU_sgn__(at::Tensor & self) {
    return at::native::sgn_sparse_(self);
}
at::Tensor & wrap_SparseNPU_sgn_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::sgn_sparse_out(self, out);
}
at::Tensor & wrap_SparseNPU_conj_physical_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::conj_physical_out_sparse(self, out);
}
at::Tensor wrap_SparseNPU_add_Tensor(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha=1) {
    return at::native::add_sparse(self, other, alpha);
}
at::Tensor & wrap_SparseNPU_add__Tensor(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha=1) {
    return at::native::add_sparse_(self, other, alpha);
}
at::Tensor wrap_SparseNPU_asinh_(const at::Tensor & self) {
    return at::native::asinh_sparse(self);
}
at::Tensor & wrap_SparseNPU_asinh__(at::Tensor & self) {
    return at::native::asinh_sparse_(self);
}
at::Tensor & wrap_SparseNPU_asinh_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::asinh_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_atanh_(const at::Tensor & self) {
    return at::native::atanh_sparse(self);
}
at::Tensor & wrap_SparseNPU_atanh__(at::Tensor & self) {
    return at::native::atanh_sparse_(self);
}
at::Tensor & wrap_SparseNPU_atanh_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::atanh_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_asin_(const at::Tensor & self) {
    return at::native::asin_sparse(self);
}
at::Tensor & wrap_SparseNPU_asin__(at::Tensor & self) {
    return at::native::asin_sparse_(self);
}
at::Tensor & wrap_SparseNPU_asin_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::asin_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_atan_(const at::Tensor & self) {
    return at::native::atan_sparse(self);
}
at::Tensor & wrap_SparseNPU_atan__(at::Tensor & self) {
    return at::native::atan_sparse_(self);
}
at::Tensor & wrap_SparseNPU_atan_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::atan_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU__sparse_broadcast_to_(const at::Tensor & self, at::IntArrayRef size) {
    return at::native::sparse_broadcast_to(self, size);
}
at::Tensor wrap_SparseNPU_cat_(const at::ITensorListRef & tensors, int64_t dim=0) {
    return at::native::cat_sparse(tensors, dim);
}
at::Tensor wrap_SparseNPU_ceil_(const at::Tensor & self) {
    return at::native::ceil_sparse(self);
}
at::Tensor & wrap_SparseNPU_ceil__(at::Tensor & self) {
    return at::native::ceil_sparse_(self);
}
at::Tensor & wrap_SparseNPU_ceil_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::ceil_sparse_out(self, out);
}
at::Tensor & wrap_SparseNPU_copy__(at::Tensor & self, const at::Tensor & src, bool non_blocking=false) {
    return at::native::copy_sparse_wrapper_(self, src, non_blocking);
}
at::Tensor wrap_SparseNPU_div_Tensor(const at::Tensor & self, const at::Tensor & other) {
    return at::native::div_sparse(self, other);
}
at::Tensor & wrap_SparseNPU_div__Tensor(at::Tensor & self, const at::Tensor & other) {
    return at::native::div_sparse_(self, other);
}
at::Tensor & wrap_SparseNPU_div_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    return at::native::div_out_sparse_zerodim(self, other, out);
}
at::Tensor wrap_SparseNPU_div_Tensor_mode(const at::Tensor & self, const at::Tensor & other, ::std::optional<c10::string_view> rounding_mode) {
    return at::native::div_sparse(self, other, rounding_mode);
}
at::Tensor & wrap_SparseNPU_div__Tensor_mode(at::Tensor & self, const at::Tensor & other, ::std::optional<c10::string_view> rounding_mode) {
    return at::native::div_sparse_(self, other, rounding_mode);
}
at::Tensor & wrap_SparseNPU_div_out_mode(const at::Tensor & self, const at::Tensor & other, ::std::optional<c10::string_view> rounding_mode, at::Tensor & out) {
    return at::native::div_out_sparse_zerodim(self, other, rounding_mode, out);
}
at::Tensor wrap_SparseNPU_empty_memory_format(at::IntArrayRef size, ::std::optional<at::ScalarType> dtype={}, ::std::optional<at::Layout> layout={}, ::std::optional<at::Device> device={}, ::std::optional<bool> pin_memory={}, ::std::optional<at::MemoryFormat> memory_format=::std::nullopt) {
    return at::native::empty_sparse(size, dtype, layout, device, pin_memory, memory_format);
}
at::Tensor wrap_SparseNPU_empty_like_(const at::Tensor & self, ::std::optional<at::ScalarType> dtype={}, ::std::optional<at::Layout> layout={}, ::std::optional<at::Device> device={}, ::std::optional<bool> pin_memory={}, ::std::optional<at::MemoryFormat> memory_format=::std::nullopt) {
    return at::native::empty_like_sparse_coo(self, dtype, layout, device, pin_memory, memory_format);
}
at::Tensor wrap_SparseNPU_erf_(const at::Tensor & self) {
    return at::native::erf_sparse(self);
}
at::Tensor & wrap_SparseNPU_erf__(at::Tensor & self) {
    return at::native::erf_sparse_(self);
}
at::Tensor & wrap_SparseNPU_erf_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::erf_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_expm1_(const at::Tensor & self) {
    return at::native::expm1_sparse(self);
}
at::Tensor & wrap_SparseNPU_expm1__(at::Tensor & self) {
    return at::native::expm1_sparse_(self);
}
at::Tensor & wrap_SparseNPU_expm1_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::expm1_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_floor_(const at::Tensor & self) {
    return at::native::floor_sparse(self);
}
at::Tensor & wrap_SparseNPU_floor__(at::Tensor & self) {
    return at::native::floor_sparse_(self);
}
at::Tensor & wrap_SparseNPU_floor_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::floor_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_floor_divide_(const at::Tensor & self, const at::Tensor & other) {
    return at::native::floor_divide_sparse(self, other);
}
at::Tensor & wrap_SparseNPU_floor_divide__Tensor(at::Tensor & self, const at::Tensor & other) {
    return at::native::floor_divide_sparse_(self, other);
}
at::Tensor & wrap_SparseNPU_floor_divide_out(const at::Tensor & self, const at::Tensor & other, at::Tensor & out) {
    return at::native::floor_divide_out_sparse_zerodim(self, other, out);
}
at::Tensor wrap_SparseNPU_frac_(const at::Tensor & self) {
    return at::native::frac_sparse(self);
}
at::Tensor & wrap_SparseNPU_frac__(at::Tensor & self) {
    return at::native::frac_sparse_(self);
}
at::Tensor & wrap_SparseNPU_frac_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::frac_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_isnan_(const at::Tensor & self) {
    return at::native::isnan_sparse(self);
}
at::Tensor wrap_SparseNPU_nan_to_num_(const at::Tensor & self, ::std::optional<double> nan=::std::nullopt, ::std::optional<double> posinf=::std::nullopt, ::std::optional<double> neginf=::std::nullopt) {
    return at::native::nan_to_num_sparse(self, nan, posinf, neginf);
}
at::Tensor & wrap_SparseNPU_nan_to_num__(at::Tensor & self, ::std::optional<double> nan=::std::nullopt, ::std::optional<double> posinf=::std::nullopt, ::std::optional<double> neginf=::std::nullopt) {
    return at::native::nan_to_num_sparse_(self, nan, posinf, neginf);
}
at::Tensor & wrap_SparseNPU_nan_to_num_out(const at::Tensor & self, ::std::optional<double> nan, ::std::optional<double> posinf, ::std::optional<double> neginf, at::Tensor & out) {
    return at::native::nan_to_num_sparse_out(self, nan, posinf, neginf, out);
}
at::Tensor wrap_SparseNPU_log1p_(const at::Tensor & self) {
    return at::native::log1p_sparse(self);
}
at::Tensor & wrap_SparseNPU_log1p__(at::Tensor & self) {
    return at::native::log1p_sparse_(self);
}
at::Tensor & wrap_SparseNPU_log1p_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::log1p_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_mm_(const at::Tensor & self, const at::Tensor & mat2) {
    return at::native::_sparse_mm(self, mat2);
}
at::Tensor & wrap_SparseNPU_mm_out(const at::Tensor & self, const at::Tensor & mat2, at::Tensor & out) {
    return at::native::_sparse_mm_out(self, mat2, out);
}
at::Tensor wrap_SparseNPU_mul_Tensor(const at::Tensor & self, const at::Tensor & other) {
    return at::native::mul_sparse(self, other);
}
at::Tensor & wrap_SparseNPU_mul__Tensor(at::Tensor & self, const at::Tensor & other) {
    return at::native::mul_sparse_(self, other);
}
at::Tensor wrap_SparseNPU_mv_(const at::Tensor & self, const at::Tensor & vec) {
    return at::native::mv_sparse(self, vec);
}
at::Tensor wrap_SparseNPU_narrow_copy_(const at::Tensor & self, int64_t dim, int64_t start, int64_t length) {
    return at::native::narrow_copy_sparse(self, dim, start, length);
}
at::Tensor wrap_SparseNPU_permute_(const at::Tensor & self, at::IntArrayRef dims) {
    return at::native::permute_sparse_coo(self, dims);
}
at::Tensor wrap_SparseNPU_rad2deg_(const at::Tensor & self) {
    return at::native::rad2deg_sparse(self);
}
at::Tensor & wrap_SparseNPU_rad2deg__(at::Tensor & self) {
    return at::native::rad2deg_sparse_(self);
}
at::Tensor & wrap_SparseNPU_rad2deg_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::rad2deg_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_deg2rad_(const at::Tensor & self) {
    return at::native::deg2rad_sparse(self);
}
at::Tensor & wrap_SparseNPU_deg2rad__(at::Tensor & self) {
    return at::native::deg2rad_sparse_(self);
}
at::Tensor & wrap_SparseNPU_deg2rad_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::deg2rad_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_neg_(const at::Tensor & self) {
    return at::native::neg_sparse(self);
}
at::Tensor & wrap_SparseNPU_neg__(at::Tensor & self) {
    return at::native::neg_sparse_(self);
}
at::Tensor & wrap_SparseNPU_neg_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::neg_out_sparse(self, out);
}
at::Tensor wrap_SparseNPU_round_(const at::Tensor & self) {
    return at::native::round_sparse(self);
}
at::Tensor & wrap_SparseNPU_round__(at::Tensor & self) {
    return at::native::round_sparse_(self);
}
at::Tensor & wrap_SparseNPU_round_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::round_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_relu_(const at::Tensor & self) {
    return at::native::relu_sparse(self);
}
at::Tensor & wrap_SparseNPU_relu__(at::Tensor & self) {
    return at::native::relu_sparse_(self);
}
at::Tensor wrap_SparseNPU_sin_(const at::Tensor & self) {
    return at::native::sin_sparse(self);
}
at::Tensor & wrap_SparseNPU_sin__(at::Tensor & self) {
    return at::native::sin_sparse_(self);
}
at::Tensor & wrap_SparseNPU_sin_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::sin_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_sinh_(const at::Tensor & self) {
    return at::native::sinh_sparse(self);
}
at::Tensor & wrap_SparseNPU_sinh__(at::Tensor & self) {
    return at::native::sinh_sparse_(self);
}
at::Tensor & wrap_SparseNPU_sinh_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::sinh_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_sum_(const at::Tensor & self, ::std::optional<at::ScalarType> dtype=::std::nullopt) {
    return at::native::sum_coo(self, dtype);
}
at::Tensor wrap_SparseNPU_sum_dim_IntList(const at::Tensor & self, at::OptionalIntArrayRef dim, bool keepdim=false, ::std::optional<at::ScalarType> dtype=::std::nullopt) {
    return at::native::sum_sparse_coo(self, dim, keepdim, dtype);
}
at::Tensor wrap_SparseNPU_sqrt_(const at::Tensor & self) {
    return at::native::sqrt_sparse(self);
}
at::Tensor & wrap_SparseNPU_sqrt__(at::Tensor & self) {
    return at::native::sqrt_sparse_(self);
}
at::Tensor & wrap_SparseNPU_sqrt_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::sqrt_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_tan_(const at::Tensor & self) {
    return at::native::tan_sparse(self);
}
at::Tensor & wrap_SparseNPU_tan__(at::Tensor & self) {
    return at::native::tan_sparse_(self);
}
at::Tensor & wrap_SparseNPU_tan_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::tan_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_tanh_(const at::Tensor & self) {
    return at::native::tanh_sparse(self);
}
at::Tensor & wrap_SparseNPU_tanh__(at::Tensor & self) {
    return at::native::tanh_sparse_(self);
}
at::Tensor & wrap_SparseNPU_tanh_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::tanh_sparse_out(self, out);
}
at::Tensor & wrap_SparseNPU_threshold_backward_grad_input(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold, at::Tensor & grad_input) {
    return at::native::threshold_backward_sparse_out(grad_output, self, threshold, grad_input);
}
at::Tensor wrap_SparseNPU_threshold_backward_(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold) {
    return at::native::threshold_backward_sparse(grad_output, self, threshold);
}
at::Tensor wrap_SparseNPU_trunc_(const at::Tensor & self) {
    return at::native::trunc_sparse(self);
}
at::Tensor & wrap_SparseNPU_trunc__(at::Tensor & self) {
    return at::native::trunc_sparse_(self);
}
at::Tensor & wrap_SparseNPU_trunc_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::trunc_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_unsqueeze_(const at::Tensor & self, int64_t dim) {
    return at::native::unsqueeze_sparse(self, dim);
}
at::Tensor & wrap_SparseNPU_zeros_out(at::IntArrayRef size, at::Tensor & out) {
    return at::native::zeros_sparse_out(size, out);
}
at::Tensor wrap_SparseNPU_native_norm_(const at::Tensor & self, const at::Scalar & p=2) {
    return at::native::norm_sparse(self, p);
}
at::Tensor wrap_SparseNPU_native_norm_ScalarOpt_dim_dtype(const at::Tensor & self, const ::std::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, ::std::optional<at::ScalarType> dtype) {
    return at::native::norm_sparse(self, p, dim, keepdim, dtype);
}
at::Tensor wrap_SparseNPU_norm_ScalarOpt_dim_dtype(const at::Tensor & self, const ::std::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim, at::ScalarType dtype) {
    return at::native::sparse_dtype_norm(self, p, dim, keepdim, dtype);
}
at::Tensor wrap_SparseNPU_norm_ScalarOpt_dim(const at::Tensor & self, const ::std::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim=false) {
    return at::native::sparse_norm(self, p, dim, keepdim);
}
at::Tensor wrap_SparseNPU_clone_(const at::Tensor & self, ::std::optional<at::MemoryFormat> memory_format=::std::nullopt) {
    return at::native::clone_sparse(self, memory_format);
}
const at::Tensor & wrap_SparseNPU_resize_as_sparse__(const at::Tensor & self, const at::Tensor & the_template) {
    return at::native::resize_as_sparse_(self, the_template);
}
at::Tensor & wrap_SparseNPU_zero__(at::Tensor & self) {
    return at::native::zero_sparse_(self);
}
at::Tensor & wrap_SparseNPU_sub_out(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha, at::Tensor & out) {
    return at::native::sub_out_sparse(self, other, alpha, out);
}
at::Tensor wrap_SparseNPU_sub_Tensor(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha=1) {
    return at::native::sub_sparse(self, other, alpha);
}
at::Tensor & wrap_SparseNPU_sub__Tensor(at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha=1) {
    return at::native::sub_sparse_(self, other, alpha);
}
at::Tensor wrap_SparseNPU__sparse_coo_tensor_with_dims_(int64_t sparse_dim, int64_t dense_dim, at::IntArrayRef size, ::std::optional<at::ScalarType> dtype={}, ::std::optional<at::Layout> layout={}, ::std::optional<at::Device> device={}, ::std::optional<bool> pin_memory={}) {
    return at::native::new_with_dims_sparse(sparse_dim, dense_dim, size, dtype, layout, device, pin_memory);
}
at::Tensor wrap_SparseNPU__sparse_coo_tensor_with_dims_and_tensors_(int64_t sparse_dim, int64_t dense_dim, c10::SymIntArrayRef size, const at::Tensor & indices, const at::Tensor & values, ::std::optional<at::ScalarType> dtype={}, ::std::optional<at::Layout> layout={}, ::std::optional<at::Device> device={}, ::std::optional<bool> pin_memory={}, ::std::optional<bool> is_coalesced=::std::nullopt) {
    return at::native::new_with_dims_and_tensor_sparse_symint(sparse_dim, dense_dim, size, indices, values, dtype, layout, device, pin_memory, is_coalesced);
}
const at::Tensor & wrap_SparseNPU_sparse_resize__(const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
    return at::native::sparse_resize_(self, size, sparse_dim, dense_dim);
}
const at::Tensor & wrap_SparseNPU_sparse_resize_and_clear__(const at::Tensor & self, at::IntArrayRef size, int64_t sparse_dim, int64_t dense_dim) {
    return at::native::sparse_resize_and_clear_(self, size, sparse_dim, dense_dim);
}
at::Tensor wrap_SparseNPU_sparse_mask_(const at::Tensor & self, const at::Tensor & mask) {
    return at::native::sparse_mask(self, mask);
}
at::Tensor wrap_SparseNPU__sparse_mask_projection_(const at::Tensor & self, const at::Tensor & mask, bool accumulate_matches=false) {
    return at::native::sparse_mask_projection(self, mask, accumulate_matches);
}
at::Tensor wrap_SparseNPU__to_dense_(const at::Tensor & self, ::std::optional<at::ScalarType> dtype=::std::nullopt, ::std::optional<bool> masked_grad=::std::nullopt) {
    return at::native::sparse_to_dense(self, dtype, masked_grad);
}
int64_t wrap_SparseNPU_sparse_dim_(const at::Tensor & self) {
    return at::native::sparse_dim_sparse(self);
}
int64_t wrap_SparseNPU__dimI_(const at::Tensor & self) {
    return at::native::sparse_dim_sparse(self);
}
int64_t wrap_SparseNPU_dense_dim_(const at::Tensor & self) {
    return at::native::dense_dim_sparse(self);
}
int64_t wrap_SparseNPU__dimV_(const at::Tensor & self) {
    return at::native::dense_dim_sparse(self);
}
int64_t wrap_SparseNPU__nnz_(const at::Tensor & self) {
    return at::native::_nnz_sparse(self);
}
bool wrap_SparseNPU_is_coalesced_(const at::Tensor & self) {
    return at::native::is_coalesced_sparse(self);
}
at::Tensor wrap_SparseNPU__indices_(const at::Tensor & self) {
    return at::native::_indices_sparse(self);
}
at::Tensor wrap_SparseNPU__values_(const at::Tensor & self) {
    return at::native::_values_sparse(self);
}
at::Tensor & wrap_SparseNPU__coalesced__(at::Tensor & self, bool coalesced) {
    return at::native::_coalesced_sparse_(self, coalesced);
}
at::Tensor wrap_SparseNPU_indices_(const at::Tensor & self) {
    return at::native::indices_sparse(self);
}
at::Tensor wrap_SparseNPU_values_(const at::Tensor & self) {
    return at::native::values_sparse(self);
}
at::Tensor & wrap_SparseNPU_copy_sparse_to_sparse__(at::Tensor & self, const at::Tensor & src, bool non_blocking=false) {
    return at::native::copy_sparse_(self, src, non_blocking);
}
at::Tensor wrap_SparseNPU__to_sparse_sparse_dim(const at::Tensor & self, int64_t sparse_dim) {
    return at::native::sparse_coo_to_sparse(self, sparse_dim);
}
at::Tensor wrap_SparseNPU__to_sparse_(const at::Tensor & self, ::std::optional<at::Layout> layout=::std::nullopt, at::OptionalIntArrayRef blocksize=::std::nullopt, ::std::optional<int64_t> dense_dim=::std::nullopt) {
    return at::native::sparse_coo_to_sparse(self, layout, blocksize, dense_dim);
}
at::Tensor wrap_SparseNPU__to_sparse_csr_(const at::Tensor & self, ::std::optional<int64_t> dense_dim=::std::nullopt) {
    return at::native::coo_to_sparse_csr(self, dense_dim);
}
at::Tensor wrap_SparseNPU__to_sparse_csc_(const at::Tensor & self, ::std::optional<int64_t> dense_dim=::std::nullopt) {
    return at::native::coo_to_sparse_csc(self, dense_dim);
}
at::Tensor wrap_SparseNPU__to_sparse_bsr_(const at::Tensor & self, at::IntArrayRef blocksize, ::std::optional<int64_t> dense_dim=::std::nullopt) {
    return at::native::coo_to_sparse_bsr(self, blocksize, dense_dim);
}
at::Tensor wrap_SparseNPU__to_sparse_bsc_(const at::Tensor & self, at::IntArrayRef blocksize, ::std::optional<int64_t> dense_dim=::std::nullopt) {
    return at::native::coo_to_sparse_bsc(self, blocksize, dense_dim);
}
at::Tensor wrap_SparseNPU_erfinv_(const at::Tensor & self) {
    return at::native::erfinv_sparse(self);
}
at::Tensor & wrap_SparseNPU_erfinv__(at::Tensor & self) {
    return at::native::erfinv_sparse_(self);
}
at::Tensor & wrap_SparseNPU_erfinv_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::erfinv_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_sign_(const at::Tensor & self) {
    return at::native::sign_sparse(self);
}
at::Tensor & wrap_SparseNPU_sign__(at::Tensor & self) {
    return at::native::sign_sparse_(self);
}
at::Tensor & wrap_SparseNPU_sign_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::sign_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_signbit_(const at::Tensor & self) {
    return at::native::signbit_sparse(self);
}
at::Tensor & wrap_SparseNPU_signbit_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::signbit_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_any_(const at::Tensor & self) {
    return at::native::any_sparse(self);
}
at::Tensor & wrap_SparseNPU_pow_Tensor_Scalar_out(const at::Tensor & self, const at::Scalar & exponent, at::Tensor & out) {
    return at::native::pow_out_sparse_scalar(self, exponent, out);
}
at::Tensor wrap_SparseNPU_pow_Tensor_Scalar(const at::Tensor & self, const at::Scalar & exponent) {
    return at::native::pow_sparse_scalar(self, exponent);
}
at::Tensor wrap_SparseNPU_isinf_(const at::Tensor & self) {
    return at::native::isinf_sparse(self);
}
at::Tensor wrap_SparseNPU_isposinf_(const at::Tensor & self) {
    return at::native::isposinf_sparse(self);
}
at::Tensor & wrap_SparseNPU_isposinf_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::isposinf_sparse_out(self, out);
}
at::Tensor wrap_SparseNPU_isneginf_(const at::Tensor & self) {
    return at::native::isneginf_sparse(self);
}
at::Tensor & wrap_SparseNPU_isneginf_out(const at::Tensor & self, at::Tensor & out) {
    return at::native::isneginf_sparse_out(self, out);
}
TORCH_LIBRARY_IMPL(aten, SparsePrivateUse1, m) {
m.impl("abs", TORCH_FN(wrap_SparseNPU_abs_));
m.impl("abs_", TORCH_FN(wrap_SparseNPU_abs__));
m.impl("abs.out", TORCH_FN(wrap_SparseNPU_abs_out));
m.impl("sgn", TORCH_FN(wrap_SparseNPU_sgn_));
m.impl("sgn_", TORCH_FN(wrap_SparseNPU_sgn__));
m.impl("sgn.out", TORCH_FN(wrap_SparseNPU_sgn_out));
m.impl("conj_physical.out", TORCH_FN(wrap_SparseNPU_conj_physical_out));
m.impl("add.Tensor", TORCH_FN(wrap_SparseNPU_add_Tensor));
m.impl("add_.Tensor", TORCH_FN(wrap_SparseNPU_add__Tensor));
m.impl("asinh", TORCH_FN(wrap_SparseNPU_asinh_));
m.impl("asinh_", TORCH_FN(wrap_SparseNPU_asinh__));
m.impl("asinh.out", TORCH_FN(wrap_SparseNPU_asinh_out));
m.impl("atanh", TORCH_FN(wrap_SparseNPU_atanh_));
m.impl("atanh_", TORCH_FN(wrap_SparseNPU_atanh__));
m.impl("atanh.out", TORCH_FN(wrap_SparseNPU_atanh_out));
m.impl("asin", TORCH_FN(wrap_SparseNPU_asin_));
m.impl("asin_", TORCH_FN(wrap_SparseNPU_asin__));
m.impl("asin.out", TORCH_FN(wrap_SparseNPU_asin_out));
m.impl("atan", TORCH_FN(wrap_SparseNPU_atan_));
m.impl("atan_", TORCH_FN(wrap_SparseNPU_atan__));
m.impl("atan.out", TORCH_FN(wrap_SparseNPU_atan_out));
m.impl("_sparse_broadcast_to", TORCH_FN(wrap_SparseNPU__sparse_broadcast_to_));
m.impl("cat", TORCH_FN(wrap_SparseNPU_cat_));
m.impl("ceil", TORCH_FN(wrap_SparseNPU_ceil_));
m.impl("ceil_", TORCH_FN(wrap_SparseNPU_ceil__));
m.impl("ceil.out", TORCH_FN(wrap_SparseNPU_ceil_out));
m.impl("copy_", TORCH_FN(wrap_SparseNPU_copy__));
m.impl("div.Tensor", TORCH_FN(wrap_SparseNPU_div_Tensor));
m.impl("div_.Tensor", TORCH_FN(wrap_SparseNPU_div__Tensor));
m.impl("div.out", TORCH_FN(wrap_SparseNPU_div_out));
m.impl("div.Tensor_mode", TORCH_FN(wrap_SparseNPU_div_Tensor_mode));
m.impl("div_.Tensor_mode", TORCH_FN(wrap_SparseNPU_div__Tensor_mode));
m.impl("div.out_mode", TORCH_FN(wrap_SparseNPU_div_out_mode));
m.impl("empty.memory_format", TORCH_FN(wrap_SparseNPU_empty_memory_format));
m.impl("empty_like", TORCH_FN(wrap_SparseNPU_empty_like_));
m.impl("erf", TORCH_FN(wrap_SparseNPU_erf_));
m.impl("erf_", TORCH_FN(wrap_SparseNPU_erf__));
m.impl("erf.out", TORCH_FN(wrap_SparseNPU_erf_out));
m.impl("expm1", TORCH_FN(wrap_SparseNPU_expm1_));
m.impl("expm1_", TORCH_FN(wrap_SparseNPU_expm1__));
m.impl("expm1.out", TORCH_FN(wrap_SparseNPU_expm1_out));
m.impl("floor", TORCH_FN(wrap_SparseNPU_floor_));
m.impl("floor_", TORCH_FN(wrap_SparseNPU_floor__));
m.impl("floor.out", TORCH_FN(wrap_SparseNPU_floor_out));
m.impl("floor_divide", TORCH_FN(wrap_SparseNPU_floor_divide_));
m.impl("floor_divide_.Tensor", TORCH_FN(wrap_SparseNPU_floor_divide__Tensor));
m.impl("floor_divide.out", TORCH_FN(wrap_SparseNPU_floor_divide_out));
m.impl("frac", TORCH_FN(wrap_SparseNPU_frac_));
m.impl("frac_", TORCH_FN(wrap_SparseNPU_frac__));
m.impl("frac.out", TORCH_FN(wrap_SparseNPU_frac_out));
m.impl("isnan", TORCH_FN(wrap_SparseNPU_isnan_));
m.impl("nan_to_num", TORCH_FN(wrap_SparseNPU_nan_to_num_));
m.impl("nan_to_num_", TORCH_FN(wrap_SparseNPU_nan_to_num__));
m.impl("nan_to_num.out", TORCH_FN(wrap_SparseNPU_nan_to_num_out));
m.impl("log1p", TORCH_FN(wrap_SparseNPU_log1p_));
m.impl("log1p_", TORCH_FN(wrap_SparseNPU_log1p__));
m.impl("log1p.out", TORCH_FN(wrap_SparseNPU_log1p_out));
m.impl("mm", TORCH_FN(wrap_SparseNPU_mm_));
m.impl("mm.out", TORCH_FN(wrap_SparseNPU_mm_out));
m.impl("mul.Tensor", TORCH_FN(wrap_SparseNPU_mul_Tensor));
m.impl("mul_.Tensor", TORCH_FN(wrap_SparseNPU_mul__Tensor));
m.impl("mv", TORCH_FN(wrap_SparseNPU_mv_));
m.impl("narrow_copy", TORCH_FN(wrap_SparseNPU_narrow_copy_));
m.impl("permute", TORCH_FN(wrap_SparseNPU_permute_));
m.impl("rad2deg", TORCH_FN(wrap_SparseNPU_rad2deg_));
m.impl("rad2deg_", TORCH_FN(wrap_SparseNPU_rad2deg__));
m.impl("rad2deg.out", TORCH_FN(wrap_SparseNPU_rad2deg_out));
m.impl("deg2rad", TORCH_FN(wrap_SparseNPU_deg2rad_));
m.impl("deg2rad_", TORCH_FN(wrap_SparseNPU_deg2rad__));
m.impl("deg2rad.out", TORCH_FN(wrap_SparseNPU_deg2rad_out));
m.impl("neg", TORCH_FN(wrap_SparseNPU_neg_));
m.impl("neg_", TORCH_FN(wrap_SparseNPU_neg__));
m.impl("neg.out", TORCH_FN(wrap_SparseNPU_neg_out));
m.impl("round", TORCH_FN(wrap_SparseNPU_round_));
m.impl("round_", TORCH_FN(wrap_SparseNPU_round__));
m.impl("round.out", TORCH_FN(wrap_SparseNPU_round_out));
m.impl("relu", TORCH_FN(wrap_SparseNPU_relu_));
m.impl("relu_", TORCH_FN(wrap_SparseNPU_relu__));
m.impl("sin", TORCH_FN(wrap_SparseNPU_sin_));
m.impl("sin_", TORCH_FN(wrap_SparseNPU_sin__));
m.impl("sin.out", TORCH_FN(wrap_SparseNPU_sin_out));
m.impl("sinh", TORCH_FN(wrap_SparseNPU_sinh_));
m.impl("sinh_", TORCH_FN(wrap_SparseNPU_sinh__));
m.impl("sinh.out", TORCH_FN(wrap_SparseNPU_sinh_out));
m.impl("sum", TORCH_FN(wrap_SparseNPU_sum_));
m.impl("sum.dim_IntList", TORCH_FN(wrap_SparseNPU_sum_dim_IntList));
m.impl("sqrt", TORCH_FN(wrap_SparseNPU_sqrt_));
m.impl("sqrt_", TORCH_FN(wrap_SparseNPU_sqrt__));
m.impl("sqrt.out", TORCH_FN(wrap_SparseNPU_sqrt_out));
m.impl("tan", TORCH_FN(wrap_SparseNPU_tan_));
m.impl("tan_", TORCH_FN(wrap_SparseNPU_tan__));
m.impl("tan.out", TORCH_FN(wrap_SparseNPU_tan_out));
m.impl("tanh", TORCH_FN(wrap_SparseNPU_tanh_));
m.impl("tanh_", TORCH_FN(wrap_SparseNPU_tanh__));
m.impl("tanh.out", TORCH_FN(wrap_SparseNPU_tanh_out));
m.impl("threshold_backward.grad_input", TORCH_FN(wrap_SparseNPU_threshold_backward_grad_input));
m.impl("threshold_backward", TORCH_FN(wrap_SparseNPU_threshold_backward_));
m.impl("trunc", TORCH_FN(wrap_SparseNPU_trunc_));
m.impl("trunc_", TORCH_FN(wrap_SparseNPU_trunc__));
m.impl("trunc.out", TORCH_FN(wrap_SparseNPU_trunc_out));
m.impl("unsqueeze", TORCH_FN(wrap_SparseNPU_unsqueeze_));
m.impl("zeros.out", TORCH_FN(wrap_SparseNPU_zeros_out));
m.impl("native_norm", TORCH_FN(wrap_SparseNPU_native_norm_));
m.impl("native_norm.ScalarOpt_dim_dtype", TORCH_FN(wrap_SparseNPU_native_norm_ScalarOpt_dim_dtype));
m.impl("norm.ScalarOpt_dim_dtype", TORCH_FN(wrap_SparseNPU_norm_ScalarOpt_dim_dtype));
m.impl("norm.ScalarOpt_dim", TORCH_FN(wrap_SparseNPU_norm_ScalarOpt_dim));
m.impl("clone", TORCH_FN(wrap_SparseNPU_clone_));
m.impl("resize_as_sparse_", TORCH_FN(wrap_SparseNPU_resize_as_sparse__));
m.impl("zero_", TORCH_FN(wrap_SparseNPU_zero__));
m.impl("sub.out", TORCH_FN(wrap_SparseNPU_sub_out));
m.impl("sub.Tensor", TORCH_FN(wrap_SparseNPU_sub_Tensor));
m.impl("sub_.Tensor", TORCH_FN(wrap_SparseNPU_sub__Tensor));
m.impl("_sparse_coo_tensor_with_dims", TORCH_FN(wrap_SparseNPU__sparse_coo_tensor_with_dims_));
m.impl("_sparse_coo_tensor_with_dims_and_tensors", TORCH_FN(wrap_SparseNPU__sparse_coo_tensor_with_dims_and_tensors_));
m.impl("sparse_resize_", TORCH_FN(wrap_SparseNPU_sparse_resize__));
m.impl("sparse_resize_and_clear_", TORCH_FN(wrap_SparseNPU_sparse_resize_and_clear__));
m.impl("sparse_mask", TORCH_FN(wrap_SparseNPU_sparse_mask_));
m.impl("_sparse_mask_projection", TORCH_FN(wrap_SparseNPU__sparse_mask_projection_));
m.impl("_to_dense", TORCH_FN(wrap_SparseNPU__to_dense_));
m.impl("sparse_dim", TORCH_FN(wrap_SparseNPU_sparse_dim_));
m.impl("_dimI", TORCH_FN(wrap_SparseNPU__dimI_));
m.impl("dense_dim", TORCH_FN(wrap_SparseNPU_dense_dim_));
m.impl("_dimV", TORCH_FN(wrap_SparseNPU__dimV_));
m.impl("_nnz", TORCH_FN(wrap_SparseNPU__nnz_));
m.impl("is_coalesced", TORCH_FN(wrap_SparseNPU_is_coalesced_));
m.impl("_indices", TORCH_FN(wrap_SparseNPU__indices_));
m.impl("_values", TORCH_FN(wrap_SparseNPU__values_));
m.impl("_coalesced_", TORCH_FN(wrap_SparseNPU__coalesced__));
m.impl("indices", TORCH_FN(wrap_SparseNPU_indices_));
m.impl("values", TORCH_FN(wrap_SparseNPU_values_));
m.impl("copy_sparse_to_sparse_", TORCH_FN(wrap_SparseNPU_copy_sparse_to_sparse__));
m.impl("_to_sparse.sparse_dim", TORCH_FN(wrap_SparseNPU__to_sparse_sparse_dim));
m.impl("_to_sparse", TORCH_FN(wrap_SparseNPU__to_sparse_));
m.impl("_to_sparse_csr", TORCH_FN(wrap_SparseNPU__to_sparse_csr_));
m.impl("_to_sparse_csc", TORCH_FN(wrap_SparseNPU__to_sparse_csc_));
m.impl("_to_sparse_bsr", TORCH_FN(wrap_SparseNPU__to_sparse_bsr_));
m.impl("_to_sparse_bsc", TORCH_FN(wrap_SparseNPU__to_sparse_bsc_));
m.impl("erfinv", TORCH_FN(wrap_SparseNPU_erfinv_));
m.impl("erfinv_", TORCH_FN(wrap_SparseNPU_erfinv__));
m.impl("erfinv.out", TORCH_FN(wrap_SparseNPU_erfinv_out));
m.impl("sign", TORCH_FN(wrap_SparseNPU_sign_));
m.impl("sign_", TORCH_FN(wrap_SparseNPU_sign__));
m.impl("sign.out", TORCH_FN(wrap_SparseNPU_sign_out));
m.impl("signbit", TORCH_FN(wrap_SparseNPU_signbit_));
m.impl("signbit.out", TORCH_FN(wrap_SparseNPU_signbit_out));
m.impl("any", TORCH_FN(wrap_SparseNPU_any_));
m.impl("pow.Tensor_Scalar_out", TORCH_FN(wrap_SparseNPU_pow_Tensor_Scalar_out));
m.impl("pow.Tensor_Scalar", TORCH_FN(wrap_SparseNPU_pow_Tensor_Scalar));
m.impl("isinf", TORCH_FN(wrap_SparseNPU_isinf_));
m.impl("isposinf", TORCH_FN(wrap_SparseNPU_isposinf_));
m.impl("isposinf.out", TORCH_FN(wrap_SparseNPU_isposinf_out));
m.impl("isneginf", TORCH_FN(wrap_SparseNPU_isneginf_));
m.impl("isneginf.out", TORCH_FN(wrap_SparseNPU_isneginf_out));
}
}
} // namespace sparsenpu
} // namespace at
