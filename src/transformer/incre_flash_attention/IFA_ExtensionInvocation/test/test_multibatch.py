#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šBatchæµ‹è¯•éªŒè¯è„šæœ¬
ç”¨äºéªŒè¯ifa_v5_case.pyä¸­çš„å¤šBatché€‚é…æ˜¯å¦æ­£ç¡®
"""

import torch
import torch_npu
from torch_npu.testing.testcase import TestCase, run_tests
import custom_ops
import math
import numpy as np

def test_multibatch_setup():
    """
    æµ‹è¯•å¤šBatchè®¾ç½®æ˜¯å¦æ­£ç¡®
    """
    print("=== å¤šBatchè®¾ç½®éªŒè¯ ===")
    
    # æµ‹è¯•å‚æ•°
    batch_size = 8
    num_heads = 32
    head_dims = 128
    key_num_heads = 32
    block_size = 16
    seq_len_q = 1
    total_seq_len_kv = 32 * 1024
    block_num = total_seq_len_kv // block_size
    
    # åŸºå‡†å—æ•°
    base_block_num = (4 * 1024) // block_size
    max_actual_block_num_per_seq = base_block_num
    
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"å¤´æ•°: {num_heads}")
    print(f"é”®å¤´æ•°: {key_num_heads}")
    print(f"æ¯å¤´å—æ•°: {max_actual_block_num_per_seq}")
    print(f"å—å¤§å°: {block_size}")
    
    # è®¡ç®—å¼ é‡å½¢çŠ¶
    query_shape = [batch_size, num_heads, seq_len_q, head_dims]
    key_shape = [block_num, key_num_heads, block_size, head_dims]
    value_shape = [block_num, key_num_heads, block_size, head_dims]
    
    blocks_per_head = [max_actual_block_num_per_seq] * key_num_heads
    total_unique_blocks = sum(blocks_per_head)
    
    # äºŒç»´BlockTableå’Œä¸‰ç»´BlockPositionå½¢çŠ¶
    block_table_shape = [batch_size, total_unique_blocks]
    block_position_shape = [batch_size, key_num_heads, max_actual_block_num_per_seq]
    
    print(f"Queryå½¢çŠ¶: {query_shape}")
    print(f"Keyå½¢çŠ¶: {key_shape}")
    print(f"Valueå½¢çŠ¶: {value_shape}")
    print(f"BlockTableå½¢çŠ¶: {block_table_shape}")
    print(f"BlockPositionå½¢çŠ¶: {block_position_shape}")
    
    # éªŒè¯å½¢çŠ¶æ˜¯å¦æ­£ç¡®
    assert query_shape[0] == batch_size, f"Query batchç»´åº¦é”™è¯¯: {query_shape[0]} != {batch_size}"
    assert block_table_shape[0] == batch_size, f"BlockTable batchç»´åº¦é”™è¯¯: {block_table_shape[0]} != {batch_size}"
    assert block_position_shape[0] == batch_size, f"BlockPosition batchç»´åº¦é”™è¯¯: {block_position_shape[0]} != {batch_size}"
    
    print("âœ… å¤šBatchå½¢çŠ¶è®¾ç½®æ­£ç¡®")
    
    # æµ‹è¯•æ•°æ®è®¾ç½®
    SELECTED_BLOCK_VALUE = 0.1
    UNSELECTED_BLOCK_VALUE = 9.9
    
    query_data = torch.full(query_shape, 0.1, dtype=torch.float16)
    key_data = torch.full(key_shape, UNSELECTED_BLOCK_VALUE, dtype=torch.float16)
    value_data = torch.full(value_shape, UNSELECTED_BLOCK_VALUE, dtype=torch.float16)
    
    block_table_data = torch.full(block_table_shape, 0x7FFFFFFF, dtype=torch.int32)
    block_position_data = torch.full(block_position_shape, 0x7FFFFFFF, dtype=torch.int32)
    
    print(f"Queryæ•°æ®å½¢çŠ¶: {query_data.shape}")
    print(f"Keyæ•°æ®å½¢çŠ¶: {key_data.shape}")
    print(f"Valueæ•°æ®å½¢çŠ¶: {value_data.shape}")
    print(f"BlockTableæ•°æ®å½¢çŠ¶: {block_table_data.shape}")
    print(f"BlockPositionæ•°æ®å½¢çŠ¶: {block_position_data.shape}")
    
    # éªŒè¯æ•°æ®è®¾ç½®
    assert query_data.shape[0] == batch_size, f"Queryæ•°æ®batchç»´åº¦é”™è¯¯"
    assert block_table_data.shape[0] == batch_size, f"BlockTableæ•°æ®batchç»´åº¦é”™è¯¯"
    assert block_position_data.shape[0] == batch_size, f"BlockPositionæ•°æ®batchç»´åº¦é”™è¯¯"
    
    print("âœ… å¤šBatchæ•°æ®è®¾ç½®æ­£ç¡®")
    
    # æµ‹è¯•åºåˆ—é•¿åº¦è®¾ç½®
    actual_seq_lengths = torch.tensor([max_actual_block_num_per_seq * block_size] * batch_size, dtype=torch.int64)
    print(f"åºåˆ—é•¿åº¦å½¢çŠ¶: {actual_seq_lengths.shape}")
    print(f"åºåˆ—é•¿åº¦å€¼: {actual_seq_lengths}")
    
    assert actual_seq_lengths.shape[0] == batch_size, f"åºåˆ—é•¿åº¦batchç»´åº¦é”™è¯¯"
    assert (actual_seq_lengths == max_actual_block_num_per_seq * block_size).all(), "åºåˆ—é•¿åº¦å€¼é”™è¯¯"
    
    print("âœ… å¤šBatchåºåˆ—é•¿åº¦è®¾ç½®æ­£ç¡®")
    
    print("=== å¤šBatchè®¾ç½®éªŒè¯å®Œæˆ ===\n")

def test_multibatch_data_consistency():
    """
    æµ‹è¯•å¤šBatchæ•°æ®ä¸€è‡´æ€§
    """
    print("=== å¤šBatchæ•°æ®ä¸€è‡´æ€§éªŒè¯ ===")
    
    batch_size = 8
    key_num_heads = 32
    block_size = 16
    total_seq_len_kv = 32 * 1024
    block_num = total_seq_len_kv // block_size
    
    base_block_num = (4 * 1024) // block_size
    max_actual_block_num_per_seq = base_block_num
    
    blocks_per_head = [max_actual_block_num_per_seq] * key_num_heads
    total_unique_blocks = sum(blocks_per_head)
    
    block_table_shape = [batch_size, total_unique_blocks]
    block_position_shape = [batch_size, key_num_heads, max_actual_block_num_per_seq]
    
    block_table_data = torch.full(block_table_shape, 0x7FFFFFFF, dtype=torch.int32)
    block_position_data = torch.full(block_position_shape, 0x7FFFFFFF, dtype=torch.int32)
    
    # æ¨¡æ‹Ÿæ•°æ®è®¾ç½®é€»è¾‘
    head_block_indices = {}
    for h in range(key_num_heads):
        base_offset = (h * 3) % block_num
        head_block_indices[h] = [(base_offset + i * 2) % block_num for i in range(blocks_per_head[h])]
    
    # è®¾ç½®BlockTableå’ŒBlockPosition - ä¸ºæ‰€æœ‰batchè®¾ç½®ç›¸åŒçš„æ•°æ®
    block_table_idx = 0
    for h in range(key_num_heads):
        block_indices_tensor = torch.tensor(head_block_indices[h], dtype=torch.int32)
        
        for i in range(len(head_block_indices[h])):
            for batch_idx in range(batch_size):
                block_table_data[batch_idx, block_table_idx] = block_indices_tensor[i]
                block_position_data[batch_idx, h, i] = torch.tensor(block_table_idx, dtype=torch.int32)
            block_table_idx += 1
    
    # éªŒè¯æ‰€æœ‰batchçš„æ•°æ®æ˜¯å¦ä¸€è‡´
    for batch_idx in range(1, batch_size):
        assert torch.equal(block_table_data[batch_idx], block_table_data[0]), f"Batch {batch_idx} çš„BlockTableæ•°æ®ä¸Batch 0ä¸ä¸€è‡´"
        assert torch.equal(block_position_data[batch_idx], block_position_data[0]), f"Batch {batch_idx} çš„BlockPositionæ•°æ®ä¸Batch 0ä¸ä¸€è‡´"
    
    print("âœ… æ‰€æœ‰Batchçš„æ•°æ®è®¾ç½®ä¸€è‡´")
    
    # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
    assert not (block_table_data == 0x7FFFFFFF).all(), "BlockTableæ•°æ®æœªæ­£ç¡®è®¾ç½®"
    assert not (block_position_data == 0x7FFFFFFF).all(), "BlockPositionæ•°æ®æœªæ­£ç¡®è®¾ç½®"
    
    print("âœ… æ•°æ®æœ‰æ•ˆæ€§éªŒè¯é€šè¿‡")
    
    print("=== å¤šBatchæ•°æ®ä¸€è‡´æ€§éªŒè¯å®Œæˆ ===\n")

if __name__ == "__main__":
    print("å¼€å§‹å¤šBatché€‚é…éªŒè¯...\n")
    
    try:
        test_multibatch_setup()
        test_multibatch_data_consistency()
        print("ğŸ‰ å¤šBatché€‚é…éªŒè¯å…¨éƒ¨é€šè¿‡ï¼")
    except Exception as e:
        print(f"âŒ å¤šBatché€‚é…éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
