import torch
import torch_npu
import custom_ops # pybindå¯¼å…¥éº»çƒ¦

def test_incre_flash_attention_v4():
    """
    æµ‹è¯• incre_flash_attention_v4 ç®—å­çš„ç®€å•è°ƒç”¨
    åŸºäº ifa_v5_case.py çš„å®ç°ç»“æ„
    """
    print("å¼€å§‹æµ‹è¯• incre_flash_attention_v4...")
    
    # è®¾ç½®è®¾å¤‡
    torch.npu.set_device(0)
    
    # åŸºç¡€å‚æ•°é…ç½®
    batch_size = 1
    num_heads = 32
    key_num_heads = 8
    head_dims = 128
    block_size = 128
    total_seq_len_kv = 16 * 1024
    block_num = total_seq_len_kv // block_size
    
    # å®é™…ä½¿ç”¨çš„å—æ•°ï¼ˆç¨€ç–åœºæ™¯ï¼‰
    max_actual_block_num_per_seq = (16 * 1024) // block_size
    
    print(f"æµ‹è¯•å‚æ•°: batch_size={batch_size}, num_heads={num_heads}, key_num_heads={key_num_heads}")
    print(f"head_dims={head_dims}, block_size={block_size}, total_seq_len_kv={total_seq_len_kv}")
    print(f"å®é™…ä½¿ç”¨å—æ•°: {max_actual_block_num_per_seq}")
    
    # å¼ é‡å½¢çŠ¶å®šä¹‰
    query_shape = [batch_size, num_heads,1, head_dims]
    key_shape = [block_num, block_size, key_num_heads * head_dims]  # v4æ ¼å¼: (blocknum, blocksize, H*D)
    value_shape = [block_num, block_size, key_num_heads * head_dims]
    
    # BlockTableå’ŒblockPositionå½¢çŠ¶
    total_unique_blocks = key_num_heads * max_actual_block_num_per_seq
    block_table_shape = [batch_size, total_unique_blocks]
    block_position_shape = [batch_size, key_num_heads, max_actual_block_num_per_seq]
    
    print(f"å¼ é‡å½¢çŠ¶: query={query_shape}, key={key_shape}, value={value_shape}")
    print(f"block_table={block_table_shape}, block_position={block_position_shape}")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ® - å…¨éƒ¨ä½¿ç”¨1.0
    data_type = torch.float16
    query_data = torch.ones(query_shape, dtype=data_type)
    key_data = torch.ones(key_shape, dtype=data_type)
    value_data = torch.ones(value_shape, dtype=data_type)
    
    # BlockTableå’ŒBlockPositionæ•°æ® - ç®€åŒ–ä¸ºå…¨0
    block_table_data = torch.zeros(block_table_shape, dtype=torch.int32)
    block_position_data = torch.zeros(block_position_shape, dtype=torch.int32)
    
    # å®é™…åºåˆ—é•¿åº¦
    actual_seq_lengths = torch.tensor([max_actual_block_num_per_seq * block_size], dtype=torch.int64)
    
    # ç§»è‡³NPU
    query_npu = query_data.npu()
    key_npu = key_data.npu()
    value_npu = value_data.npu()
    block_table_npu = block_table_data.npu()
    block_position_npu = block_position_data.npu()
    seq_len_npu = actual_seq_lengths.npu()
    
    # åˆ›å»ºç©ºå¼ é‡ï¼ˆv4æ¥å£éœ€è¦ï¼‰
    empty_tensor = torch.empty(0, dtype=torch.float16).npu()
    
    # ç®—å­å‚æ•°
    scale_value = 1.0
    layout = "BNSD"
    inner_precise = 1
    
    print("è°ƒç”¨ incre_flash_attention_v4 ç®—å­...")
    
    try:
        # å¯¼å…¥è‡ªå®šä¹‰ç®—å­
        # from cann_ops_adv_vllm.extension.custom_ops.add_custom import incre_flash_attention_v4
        
        # é¢„çƒ­
        print("é¢„çƒ­é˜¶æ®µ...")
        for _ in range(5):
            output = custom_ops.incre_flash_attention_v4(
                query_npu, [key_npu], [value_npu], empty_tensor, empty_tensor,
                seq_len_npu, empty_tensor, empty_tensor, empty_tensor, empty_tensor,
                empty_tensor, empty_tensor, empty_tensor, block_table_npu, empty_tensor,
                num_heads, scale_value, layout, key_num_heads, block_size, inner_precise
            )
            
        # import ipdb; ipdb.set_trace()
        # torch_npu.npu.synchronize()
        
        # æ€§èƒ½æµ‹è¯•
        print("æ€§èƒ½æµ‹è¯•é˜¶æ®µ...")
        import time
        start = time.time()
        for _ in range(10):
            output = custom_ops.incre_flash_attention_v4(
                query_npu, [key_npu], [value_npu], empty_tensor, empty_tensor,
                seq_len_npu, empty_tensor, empty_tensor, empty_tensor, empty_tensor,
                empty_tensor, empty_tensor, empty_tensor, block_table_npu, empty_tensor,
                num_heads, scale_value, layout, key_num_heads, block_size, inner_precise
            )
        
        # torch_npu.npu.synchronize()
        end = time.time()
        
        # print(f"âœ… ç®—å­æ‰§è¡ŒæˆåŠŸ!")
        # print(f"æ‰§è¡Œæ—¶é—´: {(end - start) * 1000000:.2f} us")
        # print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # ç»“æœåˆ†æ
        # output_cpu = output.cpu().float()
        # print(f"è¾“å‡ºç»Ÿè®¡: mean={output_cpu.mean().item():.6f}, std={output_cpu.std().item():.6f}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœªé€‰ä¸­å—çš„å½±å“
        # has_unselected_value = ((output_cpu - 9.9).abs() < 1.0).any().item()
        # print(f"å—é€‰æ‹©éªŒè¯: {'âœ… é€šè¿‡' if not has_unselected_value else 'âŒ å¤±è´¥'}")
        
        return True
        
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
        return False
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œé”™è¯¯: {e}")
        return False


if __name__ == "__main__":
    success = test_incre_flash_attention_v4()
    if success:
        print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")
    else:
        print("\nğŸ’¥ æµ‹è¯•å¤±è´¥!")
        exit(1)
