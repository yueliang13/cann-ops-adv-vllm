[  4%] Generating op_host_aclnn_stub.cpp
[  4%] Generating op_host_aclnn_exc_stub.cpp
[  6%] Generating scripts/install.sh, scripts/upgrade.sh
[  9%] Building CXX object CMakeFiles/op_host_aclnnInner.dir/src/transformer/incre_flash_attention/ophost/incre_flash_attention_def.cpp.o
[ 13%] Building CXX object CMakeFiles/optiling.dir/src/utils/src/fallback_comm.cpp.o
[ 13%] Generating version.info
[ 16%] Building CXX object CMakeFiles/optiling.dir/src/transformer/incre_flash_attention/ophost/incre_flash_attention_tiling.cc.o
[ 27%] Building CXX object CMakeFiles/optiling.dir/src/transformer/incre_flash_attention/ophost/incre_flash_attention_tiling_check.cc.o
[ 27%] Building CXX object CMakeFiles/optiling.dir/src/transformer/incre_flash_attention/ophost/fallback_incre_flash_attention.cpp.o
[ 27%] Generating binary/ascend910b/src/ascendc/common
[ 27%] Generating binary/ascend910b/src/incre_flash_attention/IncreFlashAttention.py
[ 30%] Generating binary/ascend910b/bin/incre_flash_attention
[ 30%] Building CXX object CMakeFiles/optiling.dir/src/transformer/incre_flash_attention/ophost/incre_flash_attention_tiling_register.cc.o
[ 32%] Building CXX object CMakeFiles/op_host_aclnn.dir/op_host_aclnn_stub.cpp.o
[ 34%] Building CXX object CMakeFiles/op_host_aclnnExc.dir/op_host_aclnn_exc_stub.cpp.o
[ 34%] Built target incre_flash_attention_ascend910b_py_copy
[ 34%] Built target gen_version_info
[ 34%] Built target incre_flash_attention_ascend910b_mkdir
[ 34%] Built target ops_utils_inc_kernel_ascend910b
[ 34%] Built target modify_vendor
[ 37%] Generating binary/ascend910b/src/incre_flash_attention/incre_flash_attention_ascend910b_src_copy.done
[ 39%] Linking CXX shared library libop_host_aclnn.so
[ 41%] Linking CXX shared library libop_host_aclnnExc.so
[ 41%] Built target incre_flash_attention_ascend910b_src_copy
[ 41%] Built target op_host_aclnn
[ 41%] Built target op_host_aclnnExc
[ 41%] Built target opbuild_gen_default
[ 41%] Built target opbuild_gen_exc
In file included from /root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/src/transformer/incre_flash_attention/ophost/fallback_incre_flash_attention.cpp:13:
/root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/src/utils/inc/fallback.h: In function ‘fallback::aclTensor* fallback::ConvertType(const gert::Tensor*)’:
/root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/src/utils/inc/fallback.h:193:8: warning: unused variable ‘tensor_place’ [-Wunused-variable]
  193 |   auto tensor_place = ge_tensor->GetPlacement();
      |        ^~~~~~~~~~~~
/root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/src/transformer/incre_flash_attention/ophost/fallback_incre_flash_attention.cpp: In lambda function:
/root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/src/utils/inc/fallback.h:480:14: warning: declaration of ‘api_ret’ shadows a previous local [-Wshadow]
  480 |         auto api_ret = opApiFunc(workspace_addr, workspace_size, executor, acl_stream);                              \
      |              ^~~~~~~
/root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/src/transformer/incre_flash_attention/ophost/fallback_incre_flash_attention.cpp:101:9: note: in expansion of macro ‘EXEC_OPAPI_CMD’
  101 |         EXEC_OPAPI_CMD(aclnnIncreFlashAttentionV4, query, ge_tenserListKey, ge_tenserListValue, pseShiftGe, attenMaskGe,
      |         ^~~~~~~~~~~~~~
/root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/src/transformer/incre_flash_attention/ophost/fallback_incre_flash_attention.cpp:100:10: note: shadowed declaration is here
  100 |     auto api_ret =
      |          ^~~~~~~
In file included from /root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/src/transformer/incre_flash_attention/ophost/incre_flash_attention_def.cpp:15:
/root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/src/transformer/incre_flash_attention/ophost/incre_flash_attention_def.cpp: In lambda function:
/usr/local/Ascend/ascend-toolkit/8.1/include/register/op_def_registry.h:52:64: warning: unused parameter ‘name’ [-Wunused-parameter]
   52 |       ops::OpDefFactory::OpDefRegister(#opType, [](const char *name) { return opType(#opType); })
      |                                                    ~~~~~~~~~~~~^~~~
/root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/src/transformer/incre_flash_attention/ophost/incre_flash_attention_def.cpp:417:1: note: in expansion of macro ‘OP_ADD’
  417 | OP_ADD(IncreFlashAttention);
      | ^~~~~~
/root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/src/transformer/incre_flash_attention/ophost/incre_flash_attention_tiling.cc: In member function ‘bool optiling::IFATiling::GetBmm1Tiling(const matmul_tiling::DataType&, const matmul_tiling::DataType&, uint32_t)’:
/root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/src/transformer/incre_flash_attention/ophost/incre_flash_attention_tiling.cc:1634:62: warning: unused parameter ‘qType’ [-Wunused-parameter]
 1634 | bool IFATiling::GetBmm1Tiling(const matmul_tiling::DataType &qType, const matmul_tiling::DataType &kvType,
      |                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~
/root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/src/transformer/incre_flash_attention/ophost/incre_flash_attention_tiling.cc: In member function ‘bool optiling::IFATiling::GetBmm2Tiling(const matmul_tiling::DataType&, const matmul_tiling::DataType&, uint32_t)’:
/root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/src/transformer/incre_flash_attention/ophost/incre_flash_attention_tiling.cc:1688:62: warning: unused parameter ‘qType’ [-Wunused-parameter]
 1688 | bool IFATiling::GetBmm2Tiling(const matmul_tiling::DataType &qType, const matmul_tiling::DataType &kvType,
      |                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~
[ 44%] Linking CXX shared library libop_host_aclnnInner.so
[ 44%] Built target op_host_aclnnInner
[ 46%] Built target opbuild_gen_inner
[ 53%] Building CXX object CMakeFiles/ops_aclnn.dir/autogen/inner/aclnnInner_incre_flash_attention.cpp.o
[ 53%] Building CXX object CMakeFiles/opsproto.dir/autogen/inner/incre_flash_attention_proto.cpp.o
[ 53%] Generating autogen/aic-ascend910b-ops-info.json


==============check valid for ops info start==============
==============check valid for ops info end================


Compile op info cfg successfully.
INFO: /root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/build/autogen/aic-ascend910b-ops-info.ini does not exists in this project, skip generating compile commands.
[ 53%] Built target generate_ops_info_ascend910b
[ 53%] Built target generate_adapt_py
[ 53%] Built target generate_ops_info
INFO: /root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/build/autogen/exc/aic-ascend910b-ops-info.ini does not exists in this project, skip generating compile commands.
[ 53%] Built target generate_compile_cmd_ascend910b
[ 53%] Built target generate_compile_cmd
[ 53%] Built target prepare_build
[ 55%] Linking CXX static library libops_aclnn.a
[ 58%] Built target ops_aclnn
[ 62%] Building CXX object CMakeFiles/opapi.dir/src/transformer/incre_flash_attention/ophost/aclnn_incre_flash_attention_v2.cpp.o
[ 67%] Building CXX object CMakeFiles/opapi.dir/src/transformer/incre_flash_attention/ophost/aclnn_incre_flash_attention_v3.cpp.o
[ 67%] Building CXX object CMakeFiles/opapi.dir/src/transformer/incre_flash_attention/ophost/aclnn_incre_flash_attention_v4.cpp.o
[ 69%] Building CXX object CMakeFiles/opapi.dir/src/transformer/incre_flash_attention/ophost/aclnn_incre_flash_attention_v5.cpp.o
[ 69%] Building CXX object CMakeFiles/opapi.dir/src/transformer/incre_flash_attention/ophost/aclnn_incre_flash_attention.cpp.o
/root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/src/transformer/incre_flash_attention/ophost/aclnn_incre_flash_attention.cpp: In function ‘aclnnStatus aclnnIncreFlashAttentionGetWorkspaceSize(const aclTensor*, const aclTensorList*, const aclTensorList*, const aclTensor*, const aclTensor*, const aclIntArray*, int64_t, double, char*, int64_t, const aclTensor*, uint64_t*, aclOpExecutor**)’:
/root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/src/transformer/incre_flash_attention/ophost/aclnn_incre_flash_attention.cpp:32:99: warning: unused parameter ‘pseShift’ [-Wunused-parameter]
   32 |                                                      const aclTensorList *value, const aclTensor *pseShift,
      |                                                                                  ~~~~~~~~~~~~~~~~~^~~~~~~~
/root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/src/transformer/incre_flash_attention/ophost/aclnn_incre_flash_attention_v2.cpp: In function ‘aclnnStatus aclnnIncreFlashAttentionV2GetWorkspaceSize(const aclTensor*, const aclTensorList*, const aclTensorList*, const aclTensor*, const aclTensor*, const aclIntArray*, const aclTensor*, const aclTensor*, const aclTensor*, const aclTensor*, const aclTensor*, int64_t, double, char*, int64_t, const aclTensor*, uint64_t*, aclOpExecutor**)’:
/root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/src/transformer/incre_flash_attention/ophost/aclnn_incre_flash_attention_v2.cpp:32:100: warning: unused parameter ‘pseShift’ [-Wunused-parameter]
   32 |     const aclTensor *query, const aclTensorList *key, const aclTensorList *value, const aclTensor *pseShift,
      |                                                                                   ~~~~~~~~~~~~~~~~~^~~~~~~~
[ 72%] Linking CXX shared library libcust_opapi.so
[ 72%] Built target opapi
[ 74%] Linking CXX shared library libcust_opsproto_rt2.0.so
[ 74%] Built target opsproto
[ 76%] Linking CXX shared library libcust_opmaster_rt2.0.so
[ 76%] Built target optiling
[ 88%] Generating binary/ascend910b/gen/incre_flash_attention_ascend910b_4.done
[ 88%] Generating binary/ascend910b/gen/incre_flash_attention_ascend910b_5.done
[ 88%] Generating compat/liboptiling.so
[ 88%] Generating binary/ascend910b/gen/incre_flash_attention_ascend910b_2.done
[ 90%] Generating binary/ascend910b/gen/incre_flash_attention_ascend910b_7.done
[ 93%] Generating binary/ascend910b/gen/incre_flash_attention_ascend910b_1.done
[ 95%] Generating binary/ascend910b/gen/incre_flash_attention_ascend910b_0.done
[ 95%] Generating binary/ascend910b/gen/incre_flash_attention_ascend910b_6.done
[ 97%] Generating binary/ascend910b/gen/incre_flash_attention_ascend910b_3.done
[ascend910b] Generating IncreFlashAttention_699fe73c1f927184a4cb6d419c0f9da3 ...
[ascend910b] Generating IncreFlashAttention_78180bf94d9e40252895fc3c1ca477b3 ...
[ascend910b] Generating IncreFlashAttention_5e3b3dfc17bcd2d87fbf8055f23897c6 ...
[ascend910b] Generating IncreFlashAttention_d32458a801cd73fff30cd8ff51bbaf79 ...
[ascend910b] Generating IncreFlashAttention_94b66d6c865c54af93f1847ff6263ec2 ...
[ascend910b] Generating IncreFlashAttention_17f5813b757927a09b194884412e0598 ...
[ascend910b] Generating IncreFlashAttention_610a4c32d1f5dade5701e1286fe16d4b ...
[ascend910b] Generating IncreFlashAttention_0450c0cb39ab664039154e9f2fdcdda2 ...
[ 97%] Built target optiling_compat
Opc tool start working now, please wait for a moment.
[ascend910b] Generating IncreFlashAttention_17f5813b757927a09b194884412e0598 Done
make
[ 97%] Built target incre_flash_attention_ascend910b_1
Opc tool start working now, please wait for a moment.
[ascend910b] Generating IncreFlashAttention_699fe73c1f927184a4cb6d419c0f9da3 Done
make
[ 97%] Built target incre_flash_attention_ascend910b_7
Opc tool start working now, please wait for a moment.
[ascend910b] Generating IncreFlashAttention_94b66d6c865c54af93f1847ff6263ec2 Done
make
[ 97%] Built target incre_flash_attention_ascend910b_0
Opc tool start working now, please wait for a moment.
[ascend910b] Generating IncreFlashAttention_78180bf94d9e40252895fc3c1ca477b3 Done
make
[ 97%] Built target incre_flash_attention_ascend910b_2
Opc tool start working now, please wait for a moment.
[ascend910b] Generating IncreFlashAttention_5e3b3dfc17bcd2d87fbf8055f23897c6 Done
make
[ 97%] Built target incre_flash_attention_ascend910b_5
Opc tool start working now, please wait for a moment.
[ascend910b] Generating IncreFlashAttention_610a4c32d1f5dade5701e1286fe16d4b Done
make
[ 97%] Built target incre_flash_attention_ascend910b_6
Opc tool start working now, please wait for a moment.
[ascend910b] Generating IncreFlashAttention_0450c0cb39ab664039154e9f2fdcdda2 Done
make
[ 97%] Built target incre_flash_attention_ascend910b_3
Opc tool start working now, please wait for a moment.
[ascend910b] Generating IncreFlashAttention_d32458a801cd73fff30cd8ff51bbaf79 Done
make
[ 97%] Built target incre_flash_attention_ascend910b_4
[ 97%] Built target incre_flash_attention_ascend910b
[100%] Built target incre_flash_attention
[100%] Generating binary/ascend910b/bin/binary_info_config.json
[100%] Built target ops_config_ascend910b
[100%] Built target ops_config
[100%] Built target ops_kernel
Run CPack packaging tool...
CPack: Create package using External
CPack: Install projects
CPack: - Run preinstall target for: cann_ops_adv
CPack: - Install project: cann_ops_adv []
CPack: Create package

About to compress 18208 KB of data...
Adding files to archive named "CANN-custom_ops-7.7.t15.0-linux.aarch64.run"...
./help.info
./install.sh
./packages/vendors/customize/op_api/include/aclnn_incre_flash_attention.h
./packages/vendors/customize/op_api/include/aclnn_incre_flash_attention_v2.h
./packages/vendors/customize/op_api/include/aclnn_incre_flash_attention_v3.h
./packages/vendors/customize/op_api/include/aclnn_incre_flash_attention_v4.h
./packages/vendors/customize/op_api/include/aclnn_incre_flash_attention_v5.h
./packages/vendors/customize/op_api/lib/libcust_opapi.so
./packages/vendors/customize/op_impl/ai_core/tbe/config/ascend910b/aic-ascend910b-ops-info.json
./packages/vendors/customize/op_impl/ai_core/tbe/customize_impl/ascendc/common/dropmask.h
./packages/vendors/customize/op_impl/ai_core/tbe/customize_impl/ascendc/common/pse.h
./packages/vendors/customize/op_impl/ai_core/tbe/customize_impl/ascendc/common/util.h
./packages/vendors/customize/op_impl/ai_core/tbe/customize_impl/ascendc/incre_flash_attention/ifa_public_define.h
./packages/vendors/customize/op_impl/ai_core/tbe/customize_impl/ascendc/incre_flash_attention/incre_flash_attention.cpp
./packages/vendors/customize/op_impl/ai_core/tbe/customize_impl/ascendc/incre_flash_attention/incre_flash_attention_allvec_new.h
./packages/vendors/customize/op_impl/ai_core/tbe/customize_impl/ascendc/incre_flash_attention/incre_flash_attention_split_Bbn2s2_Us2.h
./packages/vendors/customize/op_impl/ai_core/tbe/customize_impl/dynamic/incre_flash_attention.py
./packages/vendors/customize/op_impl/ai_core/tbe/kernel/ascend910b/incre_flash_attention/IncreFlashAttention_0450c0cb39ab664039154e9f2fdcdda2.json
./packages/vendors/customize/op_impl/ai_core/tbe/kernel/ascend910b/incre_flash_attention/IncreFlashAttention_0450c0cb39ab664039154e9f2fdcdda2.o
./packages/vendors/customize/op_impl/ai_core/tbe/kernel/ascend910b/incre_flash_attention/IncreFlashAttention_17f5813b757927a09b194884412e0598.json
./packages/vendors/customize/op_impl/ai_core/tbe/kernel/ascend910b/incre_flash_attention/IncreFlashAttention_17f5813b757927a09b194884412e0598.o
./packages/vendors/customize/op_impl/ai_core/tbe/kernel/ascend910b/incre_flash_attention/IncreFlashAttention_5e3b3dfc17bcd2d87fbf8055f23897c6.json
./packages/vendors/customize/op_impl/ai_core/tbe/kernel/ascend910b/incre_flash_attention/IncreFlashAttention_5e3b3dfc17bcd2d87fbf8055f23897c6.o
./packages/vendors/customize/op_impl/ai_core/tbe/kernel/ascend910b/incre_flash_attention/IncreFlashAttention_610a4c32d1f5dade5701e1286fe16d4b.json
./packages/vendors/customize/op_impl/ai_core/tbe/kernel/ascend910b/incre_flash_attention/IncreFlashAttention_610a4c32d1f5dade5701e1286fe16d4b.o
./packages/vendors/customize/op_impl/ai_core/tbe/kernel/ascend910b/incre_flash_attention/IncreFlashAttention_699fe73c1f927184a4cb6d419c0f9da3.json
./packages/vendors/customize/op_impl/ai_core/tbe/kernel/ascend910b/incre_flash_attention/IncreFlashAttention_699fe73c1f927184a4cb6d419c0f9da3.o
./packages/vendors/customize/op_impl/ai_core/tbe/kernel/ascend910b/incre_flash_attention/IncreFlashAttention_78180bf94d9e40252895fc3c1ca477b3.json
./packages/vendors/customize/op_impl/ai_core/tbe/kernel/ascend910b/incre_flash_attention/IncreFlashAttention_78180bf94d9e40252895fc3c1ca477b3.o
./packages/vendors/customize/op_impl/ai_core/tbe/kernel/ascend910b/incre_flash_attention/IncreFlashAttention_94b66d6c865c54af93f1847ff6263ec2.json
./packages/vendors/customize/op_impl/ai_core/tbe/kernel/ascend910b/incre_flash_attention/IncreFlashAttention_94b66d6c865c54af93f1847ff6263ec2.o
./packages/vendors/customize/op_impl/ai_core/tbe/kernel/ascend910b/incre_flash_attention/IncreFlashAttention_d32458a801cd73fff30cd8ff51bbaf79.json
./packages/vendors/customize/op_impl/ai_core/tbe/kernel/ascend910b/incre_flash_attention/IncreFlashAttention_d32458a801cd73fff30cd8ff51bbaf79.o
./packages/vendors/customize/op_impl/ai_core/tbe/kernel/config/ascend910b/binary_info_config.json
./packages/vendors/customize/op_impl/ai_core/tbe/kernel/config/ascend910b/incre_flash_attention.json
./packages/vendors/customize/op_impl/ai_core/tbe/op_tiling/lib/linux/aarch64/libcust_opmaster_rt2.0.so
./packages/vendors/customize/op_impl/ai_core/tbe/op_tiling/liboptiling.so
./packages/vendors/customize/op_proto/inc/incre_flash_attention_proto.h
./packages/vendors/customize/op_proto/lib/linux/aarch64/libcust_opsproto_rt2.0.so
./packages/vendors/customize/version.info
./upgrade.sh
CRC: 3271428642
SHA256: e54b98c50918cdf1cf153e280be18931a8d9746a0d4182f650a98737b99ca891
Skipping md5sum at user request

Self-extractable archive "CANN-custom_ops-7.7.t15.0-linux.aarch64.run" successfully created.
Copy /root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/build/_CPack_Packages/Linux/External/CANN-custom_ops-7.7.t15.0-linux.aarch64.run/CANN-custom_ops-7.7.t15.0-linux.aarch64.run to /root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/build/
Copy /root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/build/_CPack_Packages/Linux/External/CANN-custom_ops-7.7.t15.0-linux.aarch64.run/CANN-custom_ops-7.7.t15.0-linux.aarch64.run to /root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/output/
CPack: - package: /root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/build/CANN-custom_ops-7.7.t15.0-linux.aarch64.run.json generated.
CPack: - package: /root/xzh_workspace/AscendIvfAttention/cann-ops-adv-release/build/CANN-custom_ops-7.7.t15.0-linux.aarch64.run generated.
