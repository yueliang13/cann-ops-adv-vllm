import torch
import torch_npu
import math

# --- 1. å‚æ•°å®šä¹‰ (è§£ç åœºæ™¯ Q=1, KV=32K, å¯ç”¨ Page Attention) ---

batch_size = 1
num_heads = 32
head_dims = 128
num_key_value_heads = 8      # MHA åœºæ™¯, ä¸ num_heads ç›¸åŒ
block_size = 128              # Page Attention çš„å—å¤§å°
seq_len_q = 1                 # è§£ç åœºæ™¯ï¼ŒQuery é•¿åº¦ä¸º 1
actual_seq_len_kv = 4 * 1024 # KV ç¼“å­˜ä¸­çš„å®é™…åºåˆ—é•¿åº¦

# ä¸º block_table è®¡ç®—æœ€å¤§æ‰€éœ€å—æ•°ï¼Œè¿™é‡ŒåŸºäºä¸€ä¸ªç†è®ºæœ€å¤§é•¿åº¦
max_supported_len = 32 * 1024 # å‡è®¾æ¨¡å‹æœ€å¤§æ”¯æŒ34K
max_blocks_per_seq = (max_supported_len + block_size - 1) // block_size

# å®šä¹‰ KV Cache å†…å­˜æ± çš„æ€»å¤§å°ï¼ˆæ€»å—æ•°ï¼‰
total_num_blocks = actual_seq_len_kv // block_size

print("--- Page Attention åœºæ™¯å‚æ•° ---")
print(f"Batch Size: {batch_size}")
print(f"Query/KV Heads: {num_heads}/{num_key_value_heads}")
print(f"Head Dims: {head_dims}")
print(f"Query Seq Len: {seq_len_q}")
print(f"KV History Len: {actual_seq_len_kv}")
print(f"Block Size: {block_size}")
print("-------------------------------")

# --- 2. æ„é€ è¾“å…¥å¼ é‡ (ä¸¥æ ¼éµå¾ªæ‰‹å†Œè¦æ±‚) ---

# Query (q): å¿…é¡»ä½¿ç”¨ "BNSD" å¸ƒå±€
# B = Batch, N = Num Heads, S = Seq Len, D = Head Dims
# Shape: (batch_size, num_heads, seq_len_q, head_dims) -> (8, 32, 1, 128)
q = torch.randn(batch_size, num_heads, seq_len_q, head_dims, dtype=torch.float16).npu()

# Key Cache (key) & Value Cache (value): ä½¿ç”¨é«˜æ€§èƒ½çš„ Page Attention å¸ƒå±€
# Shape: (total_num_blocks, num_key_value_heads, block_size, head_dims) -> (8192, 32, 128, 128)
key_cache = torch.randn(total_num_blocks, num_key_value_heads, block_size, head_dims, dtype=torch.float16).npu()
value_cache = torch.randn(total_num_blocks, num_key_value_heads, block_size, head_dims, dtype=torch.float16).npu()

# Block Table: å¿…é¡»æ˜¯ 2D Tensor
# Shape: (batch_size, max_blocks_per_seq) -> (8, 272)
block_table = torch.randint(0, total_num_blocks, (batch_size, max_blocks_per_seq), dtype=torch.int32).npu()

# Actual Sequence Lengths: å¿…é¡»æ˜¯ 1D Tensor, int64
# Shape: (batch_size,) -> (8,)
actual_seq_lengths = torch.full((batch_size,), actual_seq_len_kv, dtype=torch.int64)

# ç¼©æ”¾å› å­
scale = 1.0 / math.sqrt(head_dims)


# # --- 3. è°ƒç”¨ npu_incre_flash_attention å¹¶å¯ç”¨ Page Attention ---
# for i in range(10):
#     out = torch_npu.npu_incre_flash_attention(
#         q,
#         key_cache,
#         value_cache,
#         # --- å¯ç”¨ Page Attention çš„å…³é”®å‚æ•° ---
#         block_table=block_table,
#         actual_seq_lengths=actual_seq_lengths,
#         block_size=block_size,
#         # --- å…¶ä»–å¿…è¦å‚æ•° ---
#         num_heads=num_heads,
#         scale_value=scale,
#         input_layout="BNSD", # å¿…é¡»è®¾ä¸º BNSD
#         num_key_value_heads=num_key_value_heads
#     )


torch_npu.npu.synchronize()     
import time;
start = time.time()

out = torch_npu.npu_incre_flash_attention(
        q,
        key_cache,
        value_cache,
        # --- å¯ç”¨ Page Attention çš„å…³é”®å‚æ•° ---
        block_table=block_table,
        actual_seq_lengths=actual_seq_lengths,
        block_size=block_size,
        # --- å…¶ä»–å¿…è¦å‚æ•° ---
        num_heads=num_heads,
        scale_value=scale,
        input_layout="BNSD", # å¿…é¡»è®¾ä¸º BNSD
        num_key_value_heads=num_key_value_heads
    )

torch_npu.npu.synchronize()
end = time.time()
elapsed_us = (end - start) * 1000000
print(f"å®˜æ–¹32K æ‰§è¡Œæ—¶é—´: {elapsed_us:.2f} us")



# --- 4. æ‰“å°ç»“æœå½¢çŠ¶è¿›è¡ŒéªŒè¯ ---
print("\n--- å¼ é‡å½¢çŠ¶éªŒè¯ ---")
print(f"Input q shape (BNSD): {q.shape}")
print(f"Input key_cache shape: {key_cache.shape}")
print(f"Input value_cache shape: {value_cache.shape}")
print(f"Input block_table shape: {block_table.shape}")
print(f"Input actual_seq_lengths: {actual_seq_lengths.shape}, value: {actual_seq_lengths[0].item()}")
print(f"Output out shape: {out.shape}")
print("--------------------")


# å°†NPUç»“æœè½¬åˆ°CPU
out_cpu = out.cpu()

non_zero_heads = 0
for h in range(num_heads):
    # æå–å½“å‰å¤´çš„æ‰€æœ‰æ•°æ®
    head_output = out_cpu[:, h, :, :]
    
    # æ£€æŸ¥è¿™ä¸ªå¤´çš„æ‰€æœ‰å€¼æ˜¯å¦éƒ½æ¥è¿‘äº0ã€‚
    # æˆ‘ä»¬ä½¿ç”¨ torch.any æ¥åˆ¤æ–­æ˜¯å¦å­˜åœ¨ä»»ä½•éé›¶å…ƒç´ ã€‚
    is_non_zero = torch.any(head_output != 0)

    if is_non_zero:
        status = "âœ… éé›¶ (Not Zero)"
        non_zero_heads += 1
    else:
        status = "âŒ å…¨é›¶ (ALL ZERO)"

    # æ‰“å°æ¯ä¸ªå¤´æ‰€å±çš„GQAåˆ†ç»„ä¿¡æ¯å’ŒçŠ¶æ€
    kv_group_head = h // (num_heads // num_key_value_heads)
    print(f"Query Head {h:02d} (KV Group {kv_group_head}): {status}")

print("---------------------------------------")
if non_zero_heads == num_heads:
    print(f"\nğŸ‰ å…¨éƒ¨ {num_heads} ä¸ªå¤´éƒ½äº§ç”Ÿäº†éé›¶è¾“å‡ºï¼")
else:
    print(f"\nğŸš¨ æ³¨æ„: æœ‰ {num_heads - non_zero_heads} ä¸ªå¤´çš„ç»“æœä¸ºå…¨é›¶ï¼Œå¯èƒ½åœ¨è®¡ç®—ä¸­è¢«é”™è¯¯åœ°è·³è¿‡äº†ã€‚")



# è¾“å‡ºçš„ shape åº”è¯¥å’Œ query çš„ shape ä¸€è‡´
assert out.shape == q.shape
print("\nä»£ç ä¿®æ”¹å®Œæˆï¼Œå·²æ­£ç¡®å¯ç”¨ Page Attention åŠŸèƒ½ï¼")
