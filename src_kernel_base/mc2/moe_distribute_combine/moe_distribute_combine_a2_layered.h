/**
 * Copyright (c) 2025 Huawei Technologies Co., Ltd.
 * This file is a part of the CANN Open Software.
 * Licensed under CANN Open Software License Agreement Version 1.0 (the "License").
 * Please refer to the License for details. You may not use this file except in compliance with the License.
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED,
 * INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE.
 * See LICENSE in the root of the software repository for the full text of the License.
 */

/*!
 * \file moe_distribute_combine_a2_layered.h
 * \brief
 */
#ifndef MOE_DISTRIBUTE_COMBINE_A2_LAYERED_H
#define MOE_DISTRIBUTE_COMBINE_A2_LAYERED_H
#include "kernel_operator.h"
#include "kernel_tiling/kernel_tiling.h"
#include "moe_distribute_combine_tiling.h"
#include "moe_distribute_base.h"

namespace MoeDistributeCombineA2Impl {

#define TemplateMC2TypeA2layeredClass typename ExpandXType, typename ExpandIdxType
#define TemplateMC2TypeA2layeredFunc ExpandXType, ExpandIdxType
using namespace AscendC;
template <TemplateMC2TypeA2layeredClass>
class MoeDistributeCombineA2Layered {
public:
    constexpr static uint32_t BUFFER_NUM = 2U;                   // 多buf
    constexpr static uint32_t STATE_OFFSET = 512U;              // 状态空间偏移地址
    constexpr static uint32_t STATE_SPACE_SIZE = 1024U * 1024U;  // 1M
    constexpr static uint32_t UB_ALIGN = 32U;                   // UB按32字节对齐
    constexpr static uint32_t SELF_STATE_OFFSET = 512U * 1024U;  // 本卡状态空间偏移地址
    constexpr static uint32_t BATCH_WRITE_ITEM_OFFSET = 8U * 1024U;  // batchWriteInfo结构体地址相对于windowOut最后1M的偏移
    constexpr static uint32_t BATCH_WRITE_ITEM_SIZE = 32U;
    constexpr static uint32_t BLOCK_SIZE = 32U;
    constexpr static uint32_t B32_PER_BLOCK = 8U;
    constexpr static uint32_t B64_PER_BLOCK = 4U;
    constexpr static uint32_t SERVER_RANK_SIZE = 8U;
    constexpr static uint32_t IPC_DATA_OFFSET = 4U * 1024U * 1024U;
    constexpr static uint32_t RDMA_DATA_SIZE = 100U * 1024U * 1024U;
    constexpr static uint32_t EXTRA_TOKEN_INFO_NUM = 4U; // 专家信息 权重信息 量化Scale 到达标志位
    constexpr static uint64_t MB_SIZE = 1024UL * 1024UL;

    template <AscendC::HardEvent event>
    __aicore__ inline void SyncFunc()
    {
        int32_t eventID = static_cast<int32_t>(GetTPipePtr()->FetchEventID(event));
        AscendC::SetFlag<event>(eventID);
        AscendC::WaitFlag<event>(eventID);
    }
    template <typename T>
    inline __aicore__ T RoundUp(const T val, const T align)
    {
        static_assert(std::is_arithmetic<T>::value, "T must be an arithmetic type");
        if (align == 0 || val + align - 1 < val) {
            return val;
        }
        return (val + align - 1) / align * align;
    }

    __aicore__ inline MoeDistributeCombineA2Layered(){};
    __aicore__ inline void Init(GM_ADDR expandX, GM_ADDR expertIds, GM_ADDR expandIdx, GM_ADDR sendCount,
                                GM_ADDR scales, GM_ADDR XOut, GM_ADDR workspaceGM, TPipe *pipe,
                                const MoeDistributeCombineA2TilingData *tilingData, __gm__ void *mc2InitTiling, __gm__ void *mc2CcTiling);
    __aicore__ inline void Process();

private:
    __aicore__ inline void BuffInit();
    __aicore__ inline void SplitCoreCal();
    __aicore__ inline void AlltoAllDispatch();
    __aicore__ inline void SumToWindow();
    __aicore__ inline void SetStatus();
    __aicore__ inline void WaitDispatch();
    __aicore__ inline void AlltoAllServerDispatch();
    __aicore__ inline void SumToServer();
    __aicore__ inline void Preload();

    TPipe *tpipe_{nullptr};
    GlobalTensor<ExpandXType> expandXGlobal_;
    GlobalTensor<ExpandIdxType> expertIdsGlobal_;
    GlobalTensor<ExpandIdxType> expandIdxGlobal_;
    GlobalTensor<ExpandIdxType> sendCountGlobal_;
    GlobalTensor<ExpandIdxType> bkCountGlobal_;
    GlobalTensor<float> expandScalesGlobal_;
    GlobalTensor<ExpandXType> expandOutGlobal_;
    GlobalTensor<ExpandXType> rankWindow_;  // 用于存对端window的变量
    GlobalTensor<ExpandXType> localOutWindow_;
    GlobalTensor<ExpandXType> localInWindow_;
    GlobalTensor<uint32_t> bufferIdGlobal_;     // 用于存对端状态window的变量
    GlobalTensor<int32_t> statusSpaceGlobal_;   // win区状态位置拷入相关参数
    GlobalTensor<uint64_t> workspaceGlobal_;    // 存储batchWriteInfo结构体信息
    GlobalTensor<uint32_t> workspaceGlobal32_;  // 存储batchWriteInfo结构体信息
    GlobalTensor<int32_t> readStateGlobal_;
    GlobalTensor<int32_t> dstRankStateGlobal_;
    LocalTensor<uint64_t> batchWriteItemLocalB64;
    LocalTensor<uint32_t> batchWriteItemLocalB32;
    LocalTensor<uint32_t> recvCountLocal_;
    LocalTensor<uint32_t> expertWindowOffsetLocal_;
    LocalTensor<float> rowTmpFloatLocal_;
    LocalTensor<float> mulBufLocal_;
    LocalTensor<float> sumFloatLocal_;
    LocalTensor<ExpandIdxType> expertIdsLocal_;
    LocalTensor<float> expandScalesLocal_;
    LocalTensor<ExpandIdxType> indexCountsLocal_;
    LocalTensor<ExpandXType> tmpUb_;
    uint64_t shareAddreRank[8];
    GlobalTensor<ExpandXType> selfRankshareMemGlobal_;

    GM_ADDR windowInGM_;
    GM_ADDR windowOutGM_;
    GM_ADDR statusSpaceGm_;
    GM_ADDR expandXGM_;
    GM_ADDR expertIdsGM_;
    GM_ADDR expandIdxGM_;
    GM_ADDR sendCountGM_;
    GM_ADDR scalesGM_;
    GM_ADDR XOutGM_;

    // 分层所需的参数
    GM_ADDR shareAddrGM_;
    GM_ADDR offsetInnerGM_;
    GM_ADDR countInnerGM_;
    GM_ADDR offsetOuterGM_;
    GM_ADDR countOuterGM_;
    GM_ADDR recvCountInnerGM_;
    GlobalTensor<int32_t> shareAddrGlobal_;
    GlobalTensor<int64_t> shareFlagGlobal_;
    GlobalTensor<ExpandXType> shareMemGlobal_;
    GlobalTensor<ExpandXType> dstshareMemGlobal_;
    GlobalTensor<int32_t> offsetInnerGlobal_;
    GlobalTensor<int32_t> countInnerGlobal_;
    GlobalTensor<int32_t> offsetOuterGlobal_;
    GlobalTensor<int32_t> countOuterGlobal_;
    GlobalTensor<int32_t> recvCountInnerGlobal_;
    TBuf<> offsetReduceBuf_;
    TBuf<> countReduceBuf_;
    // tiling侧已确保数据上限，相乘不会越界，因此统一采用uint32_t进行处理
    uint32_t countReL{0};
    uint32_t axisBS_{0};
    uint32_t globalBs{0};
    uint32_t axisH_{0};
    uint32_t axisK_{0};  // topK
    uint32_t aivNum_{0};
    uint32_t worldSize_{0};
    uint32_t rankId_{0};
    uint32_t coreIdx_{0};              // aiv id
    uint32_t sharedExpertRankNum_{0};  // 共享专家卡数
    __gm__ HcclOpResParam *winContext_{nullptr};
    uint32_t moeExpertNum_{0};       // moe专家数, 等于worldSize_ - 共享专家卡数
    uint32_t localMoeExpertNum_{0};  // 每张卡的专家数
    uint32_t expandXRows_;
    uint64_t rankSizeOnWin_{0};
    Hccl<HCCL_SERVER_TYPE_AICPU> hccl_;
    uint64_t dataOffsetOnWin_{0};
    uint64_t stateOffsetOnWin_{0};
    uint32_t axisHFloatSize_{0};
    uint32_t axisHExpandXTypeSize_{0};
    uint32_t startRankId_{0};
    uint32_t endRankId_{0};
    uint32_t sendRankNum_{0};
    uint32_t halfWinSize_{0};
    uint32_t dataSpaceSize_{0};
    uint32_t bufferId_{0};
    uint32_t tokenNumPerCore_{0};
    uint32_t tokenIndex_{0};
    uint32_t serverNum{0};
    uint32_t ipcSliceSize{0};
    uint32_t ipcSliceNodeSize{0};
    uint64_t send_counts_inner_offset{0};
    uint64_t offset_inner_offset{0};
    uint64_t send_counts_outer_offset{0};
    uint64_t offset_outer_offset{0};
    uint64_t share_offset{0};
    uint32_t IPC_DATA_SIZE{0};
    TQueBind<QuePosition::VECIN, QuePosition::VECOUT, BUFFER_NUM> moeQueue_;
    TQue<QuePosition::VECIN, BUFFER_NUM> moeSumQueue_;
    TBuf<> rowTmpFloatBuf_;
    TBuf<> sumFloatBuf_;
    TBuf<> mulBuf_;
    TBuf<> sendCountBuf_;
    TBuf<> statusBuf_;
    TBuf<> statusSumOutBuf_;
    TBuf<> batchWriteItemBuf_;
    TBuf<> recvCountBuf_;
    TBuf<> scaleBuf_;
    TBuf<> expertWindowOffsetBuf_;
    int32_t sumTarget_{0};
    int32_t stateValue_{0};
    uint32_t startBs{0};
    uint32_t endBs{0};
    uint32_t processNum{0};
    uint32_t resNum{0};
    uint32_t resLen{0};
    uint32_t offsetIndex{0};
    uint32_t maxLocalBs{0};
    LocalTensor<int32_t> offsetReduceLocal_;
    LocalTensor<int32_t> countReduceLocal_;
};

template <TemplateMC2TypeA2layeredClass>
__aicore__ inline void MoeDistributeCombineA2Layered<TemplateMC2TypeA2layeredFunc>::Init(GM_ADDR expandX, GM_ADDR expertIds,
    GM_ADDR expandIdx, GM_ADDR sendCount, GM_ADDR scales, GM_ADDR XOut, GM_ADDR workspaceGM, TPipe *pipe,
    const MoeDistributeCombineA2TilingData *tilingData, __gm__ void *mc2InitTiling, __gm__ void *mc2CcTiling)
{
    tpipe_ = pipe;
    expandXGM_ = expandX;
    expertIdsGM_ = expertIds;
    expandIdxGM_ = expandIdx;
    sendCountGM_ = sendCount;
    scalesGM_ = scales;
    XOutGM_ = XOut;
    rankId_ = tilingData->moeDistributeCombineInfo.epRankId;
    axisBS_ = tilingData->moeDistributeCombineInfo.bs;
    globalBs = tilingData->moeDistributeCombineInfo.globalBs;
    if (globalBs >= 256U) {
        maxLocalBs = 256U;
    } else {
        maxLocalBs = globalBs;
    }
    axisH_ = tilingData->moeDistributeCombineInfo.h;
    axisK_ = tilingData->moeDistributeCombineInfo.k;
    aivNum_ = tilingData->moeDistributeCombineInfo.aivNum;
    moeExpertNum_ = tilingData->moeDistributeCombineInfo.moeExpertNum;
    worldSize_ = tilingData->moeDistributeCombineInfo.epWorldSize;

    auto contextGM = AscendC::GetHcclContext<HCCL_GROUP_ID_0>();
    winContext_ = (__gm__ HcclOpResParam *)contextGM;
    hccl_.Init(contextGM, mc2InitTiling);
    hccl_.SetCcTiling(mc2CcTiling);

    halfWinSize_ = RDMA_DATA_SIZE / 2U;
    IPC_DATA_SIZE = winContext_->winSize - RDMA_DATA_SIZE - IPC_DATA_OFFSET;
    dataSpaceSize_ = halfWinSize_ - STATE_SPACE_SIZE;
    windowInGM_ = hccl_.GetWindowsInAddr(rankId_);
    bufferIdGlobal_.SetGlobalBuffer((__gm__ uint32_t *)(windowInGM_ + dataSpaceSize_ + worldSize_ * STATE_OFFSET));
    bufferId_ = bufferIdGlobal_(0);
    windowInGM_ = windowInGM_ + halfWinSize_ * bufferId_;
    windowOutGM_ = hccl_.GetWindowsOutAddr(rankId_) + halfWinSize_ * bufferId_;
    coreIdx_ = GetBlockIdx();
    serverNum = worldSize_ / SERVER_RANK_SIZE;
    expandXGlobal_.SetGlobalBuffer((__gm__ ExpandXType *)expandX);
    expertIdsGlobal_.SetGlobalBuffer((__gm__ ExpandIdxType *)expertIds);
    expandIdxGlobal_.SetGlobalBuffer((__gm__ ExpandIdxType *)expandIdx);
    sendCountGlobal_.SetGlobalBuffer((__gm__ int32_t *)sendCount);
    bkCountGlobal_.SetGlobalBuffer((__gm__ int32_t *)(sendCount + worldSize_ * localMoeExpertNum_ * 4));
    expandScalesGlobal_.SetGlobalBuffer((__gm__ float *)scales);
    expandOutGlobal_.SetGlobalBuffer((__gm__ ExpandXType *)XOut);
    readStateGlobal_.SetGlobalBuffer((__gm__ int32_t *)(windowOutGM_ + dataSpaceSize_));
    workspaceGlobal_.SetGlobalBuffer((__gm__ uint64_t *)(windowOutGM_ + dataSpaceSize_ + BATCH_WRITE_ITEM_OFFSET));
    workspaceGlobal32_.SetGlobalBuffer((__gm__ uint32_t *)(windowOutGM_ + dataSpaceSize_ + BATCH_WRITE_ITEM_OFFSET));
    localMoeExpertNum_ = moeExpertNum_ / worldSize_;
    expandXRows_ = localMoeExpertNum_ * axisBS_ * worldSize_;
    rankSizeOnWin_ = static_cast<uint64_t>(dataSpaceSize_ / worldSize_ / BLOCK_SIZE * BLOCK_SIZE);
    statusSpaceGm_ = windowInGM_ + dataSpaceSize_;
    statusSpaceGlobal_.SetGlobalBuffer((__gm__ int32_t *)statusSpaceGm_);
    dataOffsetOnWin_ = rankId_ * rankSizeOnWin_;
    stateOffsetOnWin_ = static_cast<uint64_t>(dataSpaceSize_ + rankId_ * STATE_OFFSET);
    axisHFloatSize_ = axisH_ * static_cast<uint32_t>(sizeof(float));
    axisHExpandXTypeSize_ = axisH_ * static_cast<uint32_t>(sizeof(ExpandXType));

    uint64_t winSizeMin = moeExpertNum_ * axisBS_ * (axisHExpandXTypeSize_ + EXTRA_TOKEN_INFO_NUM * axisK_ * sizeof(uint32_t)) + 
        IPC_DATA_OFFSET + RDMA_DATA_SIZE; // 考虑负载极其不均衡时，HCCL BUFFSIZE需要开的大小
    assert(winContext_->winSize >= winSizeMin, "The HCCL_BUFFSIZE is %lluMB, the min value should be %lluMB. \
        epWorldSize:%u, epRankId:%u, moeExpertNum:%u, globalBs:%u, bs:%u, k:%u, h:%u, aivNum:%u, \
        totalUbSize:%llu, hcclBufferSize:%u\n", 
        winContext_->winSize / MB_SIZE, winSizeMin / MB_SIZE, 
        tilingData->moeDistributeCombineInfo.epWorldSize, tilingData->moeDistributeCombineInfo.epRankId, tilingData->moeDistributeCombineInfo.moeExpertNum, 
        tilingData->moeDistributeCombineInfo.globalBs, tilingData->moeDistributeCombineInfo.bs, tilingData->moeDistributeCombineInfo.k, 
        tilingData->moeDistributeCombineInfo.h, tilingData->moeDistributeCombineInfo.aivNum, tilingData->moeDistributeCombineInfo.totalUbSize, 
        tilingData->moeDistributeCombineInfo.hcclBufferSize
    );

    GlobalTensor<int32_t> selfStatusTensor;
    selfStatusTensor.SetGlobalBuffer((__gm__ int32_t *)(statusSpaceGm_ + SELF_STATE_OFFSET));
    // coreIdx_ < serverNum
    int32_t state = selfStatusTensor(coreIdx_ * UB_ALIGN);
    if (state == 0) {
        sumTarget_ = static_cast<int32_t>(1);
        selfStatusTensor(coreIdx_ * UB_ALIGN) = 1;
        stateValue_ = 1;
    } else {
        sumTarget_ = 0;
        selfStatusTensor(coreIdx_ * UB_ALIGN) = 0;
        stateValue_ = 0;
    }
    BuffInit();
    SplitCoreCal();
    if (coreIdx_ == 0U) {
        readStateGlobal_.SetValue(0, stateValue_);
        DataCacheCleanAndInvalid<int32_t, AscendC::CacheLine::SINGLE_CACHE_LINE, AscendC::DcciDst::CACHELINE_OUT>(
            readStateGlobal_);
    }
    send_counts_inner_offset = static_cast<uint64_t>(worldSize_ * localMoeExpertNum_);
    offset_inner_offset = send_counts_inner_offset + static_cast<uint64_t>(globalBs * serverNum);
    send_counts_outer_offset = offset_inner_offset + static_cast<uint64_t>(globalBs * axisK_ * serverNum);
    offset_outer_offset = send_counts_outer_offset + static_cast<uint64_t>(axisBS_);
    share_offset = offset_outer_offset + static_cast<uint64_t>(axisBS_ * serverNum);

    shareAddrGM_ = sendCount + share_offset;
    offsetInnerGM_ = sendCount + offset_inner_offset;
    countInnerGM_ = sendCount + send_counts_inner_offset;
    offsetOuterGM_ = sendCount + offset_outer_offset;
    countOuterGM_ = sendCount + send_counts_outer_offset;

    shareAddrGlobal_.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(sendCount) + share_offset);
    offsetInnerGlobal_.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(sendCount) + offset_inner_offset);
    countInnerGlobal_.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(sendCount) + send_counts_inner_offset);
    offsetOuterGlobal_.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(sendCount) + offset_outer_offset);
    countOuterGlobal_.SetGlobalBuffer(reinterpret_cast<__gm__ int32_t *>(sendCount) + send_counts_outer_offset);

    LocalTensor<ExpandIdxType> sendCountLocal = sendCountBuf_.Get<int32_t>();
    DataCopy(sendCountLocal, shareAddrGlobal_, RoundUp(SERVER_RANK_SIZE * 2, B32_PER_BLOCK));  // 16
    PipeBarrier<PIPE_ALL>();
    for (int i = 0; i < 8; i++) {
        shareAddreRank[i] = reinterpret_cast<uint64_t>(
            RDMA_DATA_SIZE + hccl_.GetWindowsInAddr(rankId_ / SERVER_RANK_SIZE * SERVER_RANK_SIZE + i));
    }
}

template <TemplateMC2TypeA2layeredClass>
__aicore__ inline void MoeDistributeCombineA2Layered<TemplateMC2TypeA2layeredFunc>::BuffInit()
{
    tpipe_->InitBuffer(scaleBuf_, 4 * maxLocalBs * sizeof(float)); // 4k
    tpipe_->InitBuffer(moeQueue_, BUFFER_NUM, (axisHExpandXTypeSize_ + 32U));  // 7168 * 2 * 2 = 28672
    tpipe_->InitBuffer(statusBuf_, worldSize_ * UB_ALIGN);
    tpipe_->InitBuffer(rowTmpFloatBuf_, axisHFloatSize_);
    tpipe_->InitBuffer(mulBuf_, axisHFloatSize_);       //  // 7168 * 4 = 28672
    tpipe_->InitBuffer(sumFloatBuf_, axisHFloatSize_);  //  // 7168 * 4 = 28672
    tpipe_->InitBuffer(sendCountBuf_, RoundUp(moeExpertNum_, B32_PER_BLOCK) * sizeof(int32_t));  // 保存累计出现次数，用于计算windows中的偏移
    tpipe_->InitBuffer(moeSumQueue_, BUFFER_NUM, (axisHExpandXTypeSize_ + 32U));
    tpipe_->InitBuffer(statusSumOutBuf_, sizeof(float));
    tpipe_->InitBuffer(batchWriteItemBuf_, BATCH_WRITE_ITEM_SIZE * worldSize_);
    tpipe_->InitBuffer(offsetReduceBuf_, RoundUp(maxLocalBs * axisK_ * 4, (uint32_t)UB_ALIGN)); // 8k
    tpipe_->InitBuffer(countReduceBuf_, (maxLocalBs + 8) * 4); // 1k
    batchWriteItemLocalB64 = batchWriteItemBuf_.Get<uint64_t>();
    batchWriteItemLocalB32 = batchWriteItemLocalB64.template ReinterpretCast<uint32_t>();
}
template <TemplateMC2TypeA2layeredClass>
__aicore__ inline void MoeDistributeCombineA2Layered<TemplateMC2TypeA2layeredFunc>::SplitCoreCal()
{
    // 对worldSize按卡分核，得到每个核上处理的卡的数量
    sendRankNum_ = worldSize_ / aivNum_;
    uint32_t remainderRankNum = worldSize_ % aivNum_;
    startRankId_ = sendRankNum_ * coreIdx_;
    if (coreIdx_ < remainderRankNum) {
        sendRankNum_++;
        startRankId_ += coreIdx_;
    } else {
        startRankId_ += remainderRankNum;
    }
    endRankId_ = startRankId_ + sendRankNum_;
}
template <TemplateMC2TypeA2layeredClass>
__aicore__ inline void MoeDistributeCombineA2Layered<TemplateMC2TypeA2layeredFunc>::AlltoAllDispatch()
{
    rowTmpFloatLocal_ = rowTmpFloatBuf_.Get<float>();
    ipcSliceSize = IPC_DATA_SIZE / worldSize_;
    ipcSliceNodeSize = ipcSliceSize * SERVER_RANK_SIZE;
    LocalTensor<ExpandIdxType> sendCountLocal = sendCountBuf_.Get<int32_t>();
    expandScalesLocal_ = scaleBuf_.Get<float>();
    DataCopy(sendCountLocal, sendCountGlobal_, RoundUp(moeExpertNum_, B32_PER_BLOCK));
    SyncFunc<AscendC::HardEvent::MTE2_S>();
    for (uint32_t dstRankId = startRankId_; dstRankId < endRankId_; ++dstRankId) {
        // dstRankId 在本机上的同号卡
        uint32_t targetRank = dstRankId % SERVER_RANK_SIZE;
        // 计算要发往的目标IPC的地址，不考虑flag偏移
        uint64_t targetRankShareAddr = shareAddreRank[targetRank];
        uint64_t targetRankAddr = targetRankShareAddr + static_cast<uint64_t>(dstRankId / SERVER_RANK_SIZE * ipcSliceNodeSize + 
                                                                              rankId_ % SERVER_RANK_SIZE * ipcSliceSize + 
                                                                              IPC_DATA_OFFSET);

        dstshareMemGlobal_.SetGlobalBuffer((__gm__ ExpandXType *)(targetRankAddr));
        shareFlagGlobal_.SetGlobalBuffer((__gm__ int64_t *)targetRankShareAddr);
        // 计算要发送的token数量
        uint32_t rankTokenNum = 0U;

        for (uint32_t expertId = 0U; expertId < localMoeExpertNum_; ++expertId) {
            uint32_t preCount = 0U;
            if (expertId != 0U || dstRankId != 0U) {
                preCount = static_cast<uint32_t>(sendCountLocal.GetValue(expertId * worldSize_ + dstRankId - 1));
            }

            uint32_t tokenNum = sendCountLocal.GetValue(expertId * worldSize_ + dstRankId) - preCount;
            uint32_t startTokenAddr = preCount * axisH_;
            DataCopy(expandScalesLocal_, expandScalesGlobal_[preCount], (tokenNum + 31) / 32 * 32);
            SyncFunc<AscendC::HardEvent::MTE2_S>();
            for (uint32_t tokenId = 0U; tokenId < tokenNum; ++tokenId) {
                float scaleVal = expandScalesLocal_.GetValue(tokenId);
                LocalTensor<ExpandXType> InUb = moeQueue_.AllocTensor<ExpandXType>();
                LocalTensor<float> InUbTemp = InUb[axisH_].template ReinterpretCast<float>();
                InUbTemp(0) = scaleVal;
                SyncFunc<AscendC::HardEvent::S_MTE2>();
                DataCopy(InUb, expandXGlobal_[startTokenAddr], axisH_);
                moeQueue_.EnQue(InUb);
                LocalTensor<ExpandXType> OutUb = moeQueue_.DeQue<ExpandXType>();
                DataCopy(dstshareMemGlobal_[rankTokenNum * (axisH_ + 16U)], OutUb, axisH_ + 16U);
                moeQueue_.FreeTensor<ExpandXType>(OutUb);
                startTokenAddr += axisH_;
                rankTokenNum++;
                PipeBarrier<PIPE_ALL>();
            }
        }
        PipeBarrier<PIPE_ALL>();
        LocalTensor<int64_t> InUb = statusBuf_.AllocTensor<int64_t>();
        InUb.SetValue(0, 12345);
        uint32_t flagOffset = rankId_ % SERVER_RANK_SIZE + dstRankId / SERVER_RANK_SIZE * SERVER_RANK_SIZE;
        DataCopy(shareFlagGlobal_[flagOffset * 4], InUb, 4);  // *4是因为单次拷贝256byte = 4*int64
        statusBuf_.FreeTensor<int64_t>(InUb);
    }
    SyncAll<true>();
}

template <TemplateMC2TypeA2layeredClass>
__aicore__ inline void MoeDistributeCombineA2Layered<TemplateMC2TypeA2layeredFunc>::SumToWindow()
{
    // 当前假设一个core处理一个rank的数据累加，因为已经只剩下同号卡，所以只有serverNum个rank
    if (coreIdx_ < serverNum) {
        shareFlagGlobal_.SetGlobalBuffer((__gm__ int64_t *)shareAddreRank[rankId_ % SERVER_RANK_SIZE]);
        LocalTensor<int64_t> InUb = statusBuf_.AllocTensor<int64_t>();
        for (uint32_t i = 0U; i < SERVER_RANK_SIZE; i++) {
            uint32_t waitFlagAddr = coreIdx_ * SERVER_RANK_SIZE + i;
            while (true) {
                DataCopy(InUb, shareFlagGlobal_[waitFlagAddr * 4], 4);
                PipeBarrier<PIPE_ALL>();
                if (InUb.GetValue(0) == 12345) {
                    break;
                }
            }
        }
        InUb.SetValue(0, 0);
        PipeBarrier<PIPE_ALL>();
        for (uint32_t i = 0U; i < SERVER_RANK_SIZE; i++) {
            DataCopy(
                shareFlagGlobal_[(coreIdx_ * SERVER_RANK_SIZE + i) * 4], InUb, 4);  // *4是因为单次拷贝256byte = 4*int64
            PipeBarrier<PIPE_V>();
        }

        statusBuf_.FreeTensor<int64_t>(InUb);
        LocalTensor<int32_t> offsetReduceLocal = offsetReduceBuf_.Get<int32_t>();
        LocalTensor<int32_t> countReduceLocal = countReduceBuf_.Get<int32_t>();
        DataCopy(offsetReduceLocal,
                 offsetInnerGlobal_[globalBs * axisK_ * coreIdx_],
                 RoundUp(maxLocalBs * axisK_, (uint32_t)(UB_ALIGN / sizeof(int32_t))));
        DataCopy(countReduceLocal,
                 countInnerGlobal_[globalBs * coreIdx_],
                 RoundUp(maxLocalBs, (uint32_t)(UB_ALIGN / sizeof(int32_t))));
        SyncFunc<AscendC::HardEvent::MTE2_S>();

        uint64_t copyAddr = shareAddreRank[rankId_ % SERVER_RANK_SIZE] + 
                            static_cast<uint64_t>(IPC_DATA_OFFSET + coreIdx_ * ipcSliceNodeSize);
        shareMemGlobal_.SetGlobalBuffer((__gm__ ExpandXType *)copyAddr);
        uint64_t rdmaAddr = (uint64_t)(hccl_.GetWindowsOutAddr(rankId_) + halfWinSize_ * bufferId_ +
                                       coreIdx_ * rankSizeOnWin_ * SERVER_RANK_SIZE);
        localOutWindow_.SetGlobalBuffer((__gm__ ExpandXType *)rdmaAddr);
        sumFloatLocal_ = sumFloatBuf_.Get<float>();
        int offset = 0;
        int offsetPre = 0;
        countReL = axisBS_;
        offsetIndex = 0U;
        for (uint32_t i = 0U; i < maxLocalBs; i++) {
            int offsetCur = countReduceLocal.GetValue(i);
            Duplicate(sumFloatLocal_, 0.0f, axisH_);
            if (i != 0U) {
                offsetPre = countReduceLocal.GetValue(i - 1);
            }
            int copyNum = offsetCur - offsetPre;
            if (!copyNum) {
                countReL = i;
                break;
            }
            for (uint32_t j = 0U; j < static_cast<uint32_t>(copyNum); j++) {
                tmpUb_ = moeSumQueue_.AllocTensor<ExpandXType>();
                uint32_t offsetOnIpc = (offsetReduceLocal.GetValue(offsetIndex) / (globalBs * axisK_) * ipcSliceSize +
                        offsetReduceLocal.GetValue(offsetIndex) % (globalBs * axisK_) * (axisH_ + 16U) *
                        sizeof(ExpandXType)) / sizeof(ExpandXType);
                int32_t offsetInnerPos = offsetReduceLocal.GetValue(offsetIndex);
                DataCopy(tmpUb_, shareMemGlobal_[offsetOnIpc], axisH_ + 16U);
                SyncFunc<AscendC::HardEvent::MTE2_S>();
                LocalTensor<float> InUbTemp = tmpUb_[axisH_].template ReinterpretCast<float>();
                float scaleVal = InUbTemp(0);
                SyncFunc<AscendC::HardEvent::S_V>();
                moeSumQueue_.EnQue(tmpUb_);
                LocalTensor<ExpandXType> tmpOtherUb_ = moeSumQueue_.DeQue<ExpandXType>();
                Cast(rowTmpFloatLocal_, tmpOtherUb_, AscendC::RoundMode::CAST_NONE, axisH_);
                PipeBarrier<PIPE_V>();
                AscendC::Muls(rowTmpFloatLocal_, rowTmpFloatLocal_, scaleVal, axisH_);
                PipeBarrier<PIPE_V>();
                AscendC::Add(sumFloatLocal_, sumFloatLocal_, rowTmpFloatLocal_, axisH_);
                moeSumQueue_.FreeTensor<ExpandXType>(tmpOtherUb_);
                offsetIndex++;
                PipeBarrier<PIPE_V>();
            }
            PipeBarrier<PIPE_V>();
            LocalTensor<ExpandXType> castUbIn = mulBuf_.Get<ExpandXType>();
            SyncFunc<AscendC::HardEvent::MTE3_V>();
            Cast(castUbIn, sumFloatLocal_, AscendC::RoundMode::CAST_RINT, axisH_);
            SyncFunc<AscendC::HardEvent::V_MTE3>();
            DataCopy(localOutWindow_[i * axisH_], castUbIn, axisH_);
            PipeBarrier<PIPE_V>();
        }
    }

    SyncAll<true>();
}

template <TemplateMC2TypeA2layeredClass>
__aicore__ inline void MoeDistributeCombineA2Layered<TemplateMC2TypeA2layeredFunc>::AlltoAllServerDispatch()
{
    uint64_t selfTotalNum = 0U;
    if (coreIdx_ < serverNum) {
        uint32_t tragRankId = rankId_ % SERVER_RANK_SIZE + coreIdx_ * SERVER_RANK_SIZE;
        // 目标卡 GetWindowsOutAddr 地址
        uint64_t dstrdmaAddr = (uint64_t)(hccl_.GetWindowsInAddr(tragRankId) + halfWinSize_ * bufferId_ +
                                          (rankId_ / SERVER_RANK_SIZE) * rankSizeOnWin_ * SERVER_RANK_SIZE);
        uint64_t srcrdmaAddr = (uint64_t)(hccl_.GetWindowsOutAddr(rankId_) + halfWinSize_ * bufferId_ +
                                          coreIdx_ * rankSizeOnWin_ * SERVER_RANK_SIZE);

        // countReL
        batchWriteItemLocalB64(0) = srcrdmaAddr;
        batchWriteItemLocalB64(0 + 1) = dstrdmaAddr;
        if (coreIdx_ == (rankId_ / SERVER_RANK_SIZE)) {
            batchWriteItemLocalB64(0 + 2) = 0;
        } else {
            batchWriteItemLocalB64(0 + 2) = countReL * axisH_;
        }
        batchWriteItemLocalB32(0 + 6) = HcclDataType::HCCL_DATA_TYPE_FP16;
        batchWriteItemLocalB32(0 + 7) = tragRankId;

        SyncFunc<AscendC::HardEvent::S_MTE3>();
        DataCopy(workspaceGlobal_[coreIdx_ * 4], batchWriteItemLocalB64, 4);
    }
    SyncAll<true>();
    if (coreIdx_ == 0U) {
        HcclHandle handleId = hccl_.BatchWrite<true>((GM_ADDR)(workspaceGlobal_.GetPhyAddr()), serverNum);
        bufferIdGlobal_(0) = bufferId_ ^ 1;
    }
    if (coreIdx_ == (rankId_ / SERVER_RANK_SIZE)) {
        uint64_t srcrdmaAddr = (uint64_t)(hccl_.GetWindowsOutAddr(rankId_) +
                                          halfWinSize_ * bufferId_ +  //(rankId_ % SERVER_RANK_SIZE) * rankSizeOnWin_);
                                          (rankId_ / SERVER_RANK_SIZE) * rankSizeOnWin_ * SERVER_RANK_SIZE);
        uint64_t dstrdmaAddr = (uint64_t)(hccl_.GetWindowsInAddr(rankId_) +
                                          halfWinSize_ * bufferId_ +  //(rankId_ % SERVER_RANK_SIZE) * rankSizeOnWin_);
                                          (rankId_ / SERVER_RANK_SIZE) * rankSizeOnWin_ * SERVER_RANK_SIZE);

        localInWindow_.SetGlobalBuffer((__gm__ ExpandXType *)(dstrdmaAddr));
        localOutWindow_.SetGlobalBuffer((__gm__ ExpandXType *)(srcrdmaAddr));

        for (uint32_t tokenId = 0U; tokenId < countReL; ++tokenId) {
            LocalTensor<ExpandXType> InUb = moeQueue_.AllocTensor<ExpandXType>();
            DataCopy(InUb, localOutWindow_[tokenId * axisH_], axisH_);
            moeQueue_.EnQue(InUb);
            LocalTensor<ExpandXType> OutUb = moeQueue_.DeQue<ExpandXType>();
            DataCopy(localInWindow_[tokenId * axisH_], OutUb, axisH_);
            moeQueue_.FreeTensor<ExpandXType>(OutUb);
        }
    }
}

template <TemplateMC2TypeA2layeredClass>
__aicore__ inline void MoeDistributeCombineA2Layered<TemplateMC2TypeA2layeredFunc>::SetStatus()
{
    if (coreIdx_ != 0U) {
        SyncAll<true>();
        return;
    }

    uint32_t selfServerID = rankId_ / SERVER_RANK_SIZE;
    for (uint32_t serverID = 0U; serverID < serverNum; serverID++) {
        uint32_t targetRank = rankId_ % SERVER_RANK_SIZE + serverID * SERVER_RANK_SIZE;
        batchWriteItemLocalB64(serverID * 4) = (uint64_t)(readStateGlobal_.GetPhyAddr());
        batchWriteItemLocalB64(serverID * 4 + 1) =
            (uint64_t)(hccl_.GetWindowsInAddr(targetRank) + halfWinSize_ * bufferId_ + dataSpaceSize_ +
                       selfServerID * STATE_OFFSET);
        batchWriteItemLocalB64(serverID * 4 + 2) = 8;
        batchWriteItemLocalB32(serverID * 8 + 6) = HcclDataType::HCCL_DATA_TYPE_INT32;
        batchWriteItemLocalB32(serverID * 8 + 7) = targetRank;
    }
    SyncFunc<AscendC::HardEvent::S_MTE3>();
    DataCopy(workspaceGlobal_[serverNum * 4], batchWriteItemLocalB64, 4 * (serverNum));
    GlobalTensor<int32_t> localStateGlobal;
    localStateGlobal.SetGlobalBuffer((__gm__ int32_t *)(windowInGM_ + dataSpaceSize_ + selfServerID * STATE_OFFSET));
    localStateGlobal.SetValue(0, stateValue_);
    DataCacheCleanAndInvalid<int32_t, AscendC::CacheLine::SINGLE_CACHE_LINE, AscendC::DcciDst::CACHELINE_OUT>(
        localStateGlobal);
    SyncFunc<AscendC::HardEvent::MTE3_S>();
    if ASCEND_IS_AIV {
        HcclHandle handleId =
        hccl_.BatchWrite<true>((GM_ADDR)(workspaceGlobal_[serverNum * 4].GetPhyAddr()), serverNum);
    }
    SyncAll<true>();
}

template <TemplateMC2TypeA2layeredClass>
__aicore__ inline void MoeDistributeCombineA2Layered<TemplateMC2TypeA2layeredFunc>::WaitDispatch()
{
    if (coreIdx_ < serverNum) {
        uint32_t targetRank = rankId_ % SERVER_RANK_SIZE + (coreIdx_)*SERVER_RANK_SIZE;
        LocalTensor<int32_t> statusTensor = statusBuf_.Get<int32_t>();
        uint32_t readNum = 1U;
        DataCopyParams intriParams{static_cast<uint16_t>(readNum), 1, 15, 0};  // srcStride为15个block
        while (true) {
            DataCopy(statusTensor, statusSpaceGlobal_[(coreIdx_)*STATE_OFFSET / sizeof(int32_t)], intriParams);
            PipeBarrier<PIPE_ALL>();
            int32_t sumOfFlag = statusTensor.GetValue(0);

            if (sumOfFlag == sumTarget_) {
                break;
            }
        }
    }

    SyncAll<true>();
}

template <TemplateMC2TypeA2layeredClass>
__aicore__ inline void MoeDistributeCombineA2Layered<TemplateMC2TypeA2layeredFunc>::Preload()
{
    if (coreIdx_ >= 8U) {
        return;
    }
    processNum = axisBS_ / 8U;
    resNum = axisBS_ - processNum * 8U;
    resLen = (resNum == 0U) ? 0U : 1U;
    startBs = 0U;
    endBs = 0U;
    if (coreIdx_ < resNum) {
        processNum += 1U;
        startBs = coreIdx_ * processNum;
        endBs = startBs + processNum;
    } else {
        startBs = coreIdx_ * processNum + resNum;
        endBs = startBs + processNum;
    }
    uint64_t selfRankAddr = (uint64_t)(hccl_.GetWindowsInAddr(rankId_) + halfWinSize_ * bufferId_);
    localInWindow_.SetGlobalBuffer((__gm__ ExpandXType *)(selfRankAddr));
    offsetReduceLocal_ = offsetReduceBuf_.Get<int32_t>();
    countReduceLocal_ = countReduceBuf_.Get<int32_t>();
    DataCopy(
        offsetReduceLocal_, offsetOuterGlobal_, RoundUp(axisBS_ * serverNum, (uint32_t)(UB_ALIGN / sizeof(int32_t))));
    DataCopy(countReduceLocal_, countOuterGlobal_, RoundUp(axisBS_, (uint32_t)(UB_ALIGN / sizeof(int32_t))));
    SyncFunc<AscendC::HardEvent::MTE2_S>();
    offsetIndex = 0U;
    sumFloatLocal_ = sumFloatBuf_.Get<float>();

    if (startBs != 0U) {
        offsetIndex = countReduceLocal_.GetValue(startBs - 1U);
    }
}
template <TemplateMC2TypeA2layeredClass>
__aicore__ inline void MoeDistributeCombineA2Layered<TemplateMC2TypeA2layeredFunc>::SumToServer()
{
    if (coreIdx_ >= 8U) {
        SyncAll<true>();
        return;
    }
    for (uint32_t i = startBs; i < endBs; i++) {
        int offsetPre = 0;
        int offsetCur = countReduceLocal_.GetValue(i);
        if (i != 0U) {
            offsetPre = countReduceLocal_.GetValue(i - 1);
        }
        int copyNum = offsetCur - offsetPre;
        if (!copyNum) {
            break;
        }
        Duplicate(sumFloatLocal_, 0.0f, axisH_);
        for (int j = 0; j < copyNum; j++) {
            tmpUb_ = moeSumQueue_.AllocTensor<ExpandXType>();
            int offsetOnIpc = (offsetReduceLocal_.GetValue(offsetIndex) / axisBS_ * rankSizeOnWin_ * SERVER_RANK_SIZE +
                               offsetReduceLocal_.GetValue(offsetIndex) % axisBS_ * axisH_ * sizeof(ExpandXType)) /
                               sizeof(ExpandXType);
            DataCopy(tmpUb_, localInWindow_[offsetOnIpc], axisH_);
            moeSumQueue_.EnQue(tmpUb_);
            LocalTensor<ExpandXType> tmpOtherUb_ = moeSumQueue_.DeQue<ExpandXType>();
            // cast before muls
            Cast(rowTmpFloatLocal_, tmpOtherUb_, AscendC::RoundMode::CAST_NONE, axisH_);
            PipeBarrier<PIPE_V>();
            // add mulBufLocal to sumFloatBufLocal
            AscendC::Add(sumFloatLocal_, sumFloatLocal_, rowTmpFloatLocal_, axisH_);
            moeSumQueue_.FreeTensor<ExpandXType>(tmpOtherUb_);
            offsetIndex++;
        }
        PipeBarrier<PIPE_V>();
        LocalTensor<ExpandXType> castUbIn = mulBuf_.Get<ExpandXType>();
        SyncFunc<AscendC::HardEvent::MTE3_V>();
        Cast(castUbIn, sumFloatLocal_, AscendC::RoundMode::CAST_RINT, axisH_);
        SyncFunc<AscendC::HardEvent::V_MTE3>();
        DataCopy(expandOutGlobal_[i * axisH_], castUbIn, axisH_);
        PipeBarrier<PIPE_V>();
    }

    SyncAll<true>();
}

template <TemplateMC2TypeA2layeredClass>
__aicore__ inline void MoeDistributeCombineA2Layered<TemplateMC2TypeA2layeredFunc>::Process()
{
    if ASCEND_IS_AIV {
        AlltoAllDispatch();
        SumToWindow();
        AlltoAllServerDispatch();
        SetStatus();
        Preload();
        WaitDispatch();
        SumToServer();
        hccl_.Finalize();
    }
}

}  // namespace MoeDistributeCombineA2Impl
#endif  // MOE_DISTRIBUTE_COMBINE_A2_LAYERED_H
